Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 46
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.44761904761904764
At round 0 training accuracy: 0.4226530612244898
At round 0 training loss: 0.6931464076042175
gradient difference: 0.0018273826324444714
At round 0 model param: 
[array([[-1.70479887],
       [ 0.82120155],
       [ 0.72443134],
       [ 0.3254046 ],
       [ 0.1767315 ],
       [-0.00801258],
       [ 0.02610304],
       [-0.01514596]]), 0.12323914787599019]
At round 1 accuracy: 0.44761904761904764
At round 1 training accuracy: 0.4226530612244898
At round 1 training loss: 0.42225524357386995
gradient difference: 0.0008688595483571223
At round 1 model param: 
[array([[-1.87392907],
       [ 0.88531238],
       [ 0.77825765],
       [ 0.34531386],
       [ 0.17920867],
       [-0.02096448],
       [ 0.01710582],
       [-0.02382939]]), 0.16682517954281398]
At round 2 accuracy: 0.44761904761904764
At round 2 training accuracy: 0.4226530612244898
At round 2 training loss: 0.4210098683834076
gradient difference: 0.0008566203946601792
At round 2 model param: 
[array([[-1.9029179 ],
       [ 0.89552462],
       [ 0.79178527],
       [ 0.35222045],
       [ 0.18053735],
       [-0.02152358],
       [ 0.01609576],
       [-0.02522634]]), 0.19701077895505087]
At round 3 accuracy: 0.44761904761904764
At round 3 training accuracy: 0.4226530612244898
At round 3 training loss: 0.4208773076534271
gradient difference: 0.0008523809936183396
At round 3 model param: 
[array([[-1.90675928],
       [ 0.90239578],
       [ 0.79406347],
       [ 0.35395646],
       [ 0.18285198],
       [-0.02110827],
       [ 0.01713071],
       [-0.02484338]]), 0.216254432286535]
At round 4 accuracy: 0.44761904761904764
At round 4 training accuracy: 0.4226530612244898
At round 4 training loss: 0.4208834341594151
gradient difference: 0.0008535175222699176
At round 4 model param: 
[array([[-1.90889539],
       [ 0.9039345 ],
       [ 0.7930387 ],
       [ 0.35378181],
       [ 0.18068645],
       [-0.02140383],
       [ 0.01719752],
       [-0.02560274]]), 0.22637097537517548]
At round 5 accuracy: 0.44761904761904764
At round 5 training accuracy: 0.4226530612244898
At round 5 training loss: 0.42089723689215525
gradient difference: 0.0008552895207675442
At round 5 model param: 
[array([[-1.9151744 ],
       [ 0.8994545 ],
       [ 0.78856066],
       [ 0.34647275],
       [ 0.17444734],
       [-0.02871164],
       [ 0.01133971],
       [-0.0344035 ]]), 0.22660439355032785]
At round 6 accuracy: 0.44761904761904764
At round 6 training accuracy: 0.4226530612244898
At round 6 training loss: 0.42094832233020235
gradient difference: 0.0008673344807062049
At round 6 model param: 
[array([[-1.90786936],
       [ 0.90349532],
       [ 0.79433307],
       [ 0.35290922],
       [ 0.1815735 ],
       [-0.02218005],
       [ 0.01948508],
       [-0.02490575]]), 0.23590512573719025]
At round 7 accuracy: 0.44761904761904764
At round 7 training accuracy: 0.4226530612244898
At round 7 training loss: 0.4209537761551993
gradient difference: 0.0008583395945468013
At round 7 model param: 
[array([[-1.91091766],
       [ 0.90056641],
       [ 0.79063995],
       [ 0.35132094],
       [ 0.18001313],
       [-0.02246283],
       [ 0.01732378],
       [-0.02507728]]), 0.23638370633125305]
At round 8 accuracy: 0.44761904761904764
At round 8 training accuracy: 0.4226530612244898
At round 8 training loss: 0.42089795214789255
gradient difference: 0.0008562151194347721
At round 8 model param: 
[array([[-1.91130221],
       [ 0.90074236],
       [ 0.79351653],
       [ 0.34937747],
       [ 0.18241439],
       [-0.02340128],
       [ 0.01671519],
       [-0.02583843]]), 0.23771834799221583]
At round 9 accuracy: 0.44761904761904764
At round 9 training accuracy: 0.4226530612244898
At round 9 training loss: 0.42090495995112825
gradient difference: 0.0008594836129200835
At round 9 model param: 
[array([[-1.91278005],
       [ 0.89940094],
       [ 0.78986685],
       [ 0.35019633],
       [ 0.18100165],
       [-0.02310642],
       [ 0.01410185],
       [-0.03029512]]), 0.2365559275661196]
Total training time: 41.85879993438721s
At round 10 accuracy: 0.44761904761904764
At round 10 training accuracy: 0.4226530612244898
At round 10 model param: 
[array([[-1.91278005],
       [ 0.89940094],
       [ 0.78986685],
       [ 0.35019633],
       [ 0.18100165],
       [-0.02310642],
       [ 0.01410185],
       [-0.03029512]]), 0.2365559275661196]
