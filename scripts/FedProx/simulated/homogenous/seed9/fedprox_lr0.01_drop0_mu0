Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 9
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.4180952380952381
At round 0 training accuracy: 0.4502040816326531
At round 0 training loss: 0.6931464076042175
gradient difference: 0.003530169207711572
At round 0 model param: 
[array([[-1.77990862],
       [ 0.84793855],
       [ 0.6821763 ],
       [ 0.44415083],
       [ 0.17681622],
       [ 0.07388121],
       [-0.0549826 ],
       [ 0.03299152]]), -0.030034443363547325]
At round 1 accuracy: 0.4180952380952381
At round 1 training accuracy: 0.4502040816326531
At round 1 training loss: 0.41417453970227924
gradient difference: 0.0021252350733686905
At round 1 model param: 
[array([[-1.94592903],
       [ 0.94163287],
       [ 0.75218946],
       [ 0.48958755],
       [ 0.20086673],
       [ 0.07994432],
       [-0.05207381],
       [ 0.04127772]]), -0.05291443176767124]
At round 2 accuracy: 0.4180952380952381
At round 2 training accuracy: 0.4502040816326531
At round 2 training loss: 0.412705659866333
gradient difference: 0.0021046716156303716
At round 2 model param: 
[array([[-1.98152276],
       [ 0.95646104],
       [ 0.76362052],
       [ 0.49715759],
       [ 0.20403934],
       [ 0.08012985],
       [-0.05616146],
       [ 0.04013295]]), -0.07132354245654174]
At round 3 accuracy: 0.4180952380952381
At round 3 training accuracy: 0.4502040816326531
At round 3 training loss: 0.4126708720411573
gradient difference: 0.0021170605467936096
At round 3 model param: 
[array([[-1.98273914],
       [ 0.96520141],
       [ 0.77099325],
       [ 0.50279453],
       [ 0.20958378],
       [ 0.08298471],
       [-0.0473614 ],
       [ 0.04547162]]), -0.07631774619221687]
At round 4 accuracy: 0.4180952380952381
At round 4 training accuracy: 0.4502040816326531
At round 4 training loss: 0.41269167831965853
gradient difference: 0.002089128934035633
At round 4 model param: 
[array([[-1.98407698],
       [ 0.96511705],
       [ 0.77015716],
       [ 0.50345951],
       [ 0.20823894],
       [ 0.08401602],
       [-0.04673048],
       [ 0.04302481]]), -0.08298271841236524]
At round 5 accuracy: 0.4180952380952381
At round 5 training accuracy: 0.4502040816326531
At round 5 training loss: 0.4126649498939514
gradient difference: 0.0020929882456868112
At round 5 model param: 
[array([[-1.97919668],
       [ 0.97216684],
       [ 0.77410953],
       [ 0.51035235],
       [ 0.21259121],
       [ 0.08849198],
       [-0.04655748],
       [ 0.05188435]]), -0.08159003832510539]
At round 6 accuracy: 0.4180952380952381
At round 6 training accuracy: 0.4502040816326531
At round 6 training loss: 0.4128781131335667
gradient difference: 0.002075726137094659
At round 6 model param: 
[array([[-1.98748795],
       [ 0.96440245],
       [ 0.76875051],
       [ 0.50001964],
       [ 0.20463203],
       [ 0.08344235],
       [-0.0511755 ],
       [ 0.0429275 ]]), -0.08988774887153081]
At round 7 accuracy: 0.4180952380952381
At round 7 training accuracy: 0.4502040816326531
At round 7 training loss: 0.41266429850033354
gradient difference: 0.0021114821277390537
At round 7 model param: 
[array([[-1.9846867 ],
       [ 0.96438737],
       [ 0.77213137],
       [ 0.50242168],
       [ 0.20876858],
       [ 0.08503151],
       [-0.04963178],
       [ 0.04701603]]), -0.08896643881286893]
At round 8 accuracy: 0.4180952380952381
At round 8 training accuracy: 0.4502040816326531
At round 8 training loss: 0.4126622123377664
gradient difference: 0.002093210626312301
At round 8 model param: 
[array([[-1.98464225],
       [ 0.96254143],
       [ 0.77174897],
       [ 0.50200869],
       [ 0.20608767],
       [ 0.08459462],
       [-0.04877838],
       [ 0.04629184]]), -0.09043593172516141]
At round 9 accuracy: 0.4180952380952381
At round 9 training accuracy: 0.4502040816326531
At round 9 training loss: 0.41265603474208284
gradient difference: 0.002098361023992803
At round 9 model param: 
[array([[-1.98366909],
       [ 0.96674224],
       [ 0.77249722],
       [ 0.50353456],
       [ 0.20899244],
       [ 0.08721644],
       [-0.04703768],
       [ 0.04549229]]), -0.08919799806816238]
Total training time: 41.24070715904236s
At round 10 accuracy: 0.4180952380952381
At round 10 training accuracy: 0.4502040816326531
At round 10 model param: 
[array([[-1.98366909],
       [ 0.96674224],
       [ 0.77249722],
       [ 0.50353456],
       [ 0.20899244],
       [ 0.08721644],
       [-0.04703768],
       [ 0.04549229]]), -0.08919799806816238]
