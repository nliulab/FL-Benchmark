Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 22
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.44
At round 0 training accuracy: 0.43081632653061225
At round 0 training loss: 0.6931464076042175
gradient difference: 0.002800474308437923
At round 0 model param: 
[array([[-1.80925577e+00],
       [ 9.14876776e-01],
       [ 6.69941757e-01],
       [ 3.45306022e-01],
       [ 1.59584742e-01],
       [ 9.91510568e-02],
       [-2.47655170e-04],
       [ 9.44377096e-03]]), 0.06439041027000972]
At round 1 accuracy: 0.44
At round 1 training accuracy: 0.43081632653061225
At round 1 training loss: 0.4032602991376604
gradient difference: 0.0021102823962486634
At round 1 model param: 
[array([[-2.00302570e+00],
       [ 1.02363021e+00],
       [ 7.34947843e-01],
       [ 3.69793211e-01],
       [ 1.65968227e-01],
       [ 9.42259984e-02],
       [-2.61181008e-03],
       [-1.06435003e-03]]), 0.08323082327842712]
At round 2 accuracy: 0.44
At round 2 training accuracy: 0.43081632653061225
At round 2 training loss: 0.40132538761411396
gradient difference: 0.0020553459329924312
At round 2 model param: 
[array([[-2.04532773e+00],
       [ 1.04421733e+00],
       [ 7.49406142e-01],
       [ 3.71114931e-01],
       [ 1.66002574e-01],
       [ 9.26282874e-02],
       [-1.99207903e-03],
       [-2.48837790e-03]]), 0.0952955111861229]
At round 3 accuracy: 0.44
At round 3 training accuracy: 0.43081632653061225
At round 3 training loss: 0.4012386075087956
gradient difference: 0.0020432870286325434
At round 3 model param: 
[array([[-2.05809835],
       [ 1.04668544],
       [ 0.7505547 ],
       [ 0.37456227],
       [ 0.16606135],
       [ 0.09078992],
       [-0.00499339],
       [-0.0071191 ]]), 0.1008448281458446]
At round 4 accuracy: 0.44
At round 4 training accuracy: 0.43081632653061225
At round 4 training loss: 0.40124704582350595
gradient difference: 0.002024892115602509
At round 4 model param: 
[array([[-2.0604169 ],
       [ 1.04660468],
       [ 0.75097278],
       [ 0.37283828],
       [ 0.16769873],
       [ 0.08727262],
       [-0.00452161],
       [-0.00421921]]), 0.10579457134008408]
At round 5 accuracy: 0.44
At round 5 training accuracy: 0.43081632653061225
At round 5 training loss: 0.4012472757271358
gradient difference: 0.002029260147717451
At round 5 model param: 
[array([[-2.05161646e+00],
       [ 1.05654134e+00],
       [ 7.56590571e-01],
       [ 3.82915071e-01],
       [ 1.74024899e-01],
       [ 9.95970001e-02],
       [ 5.30599537e-03],
       [-9.13948087e-05]]), 0.11599663378936904]
At round 6 accuracy: 0.44
At round 6 training accuracy: 0.43081632653061225
At round 6 training loss: 0.4015399430479322
gradient difference: 0.0021168810102066052
At round 6 model param: 
[array([[-2.06103114],
       [ 1.04836832],
       [ 0.74928451],
       [ 0.37499166],
       [ 0.163855  ],
       [ 0.09318275],
       [-0.0023867 ],
       [-0.00737095]]), 0.11099009322268623]
At round 7 accuracy: 0.44
At round 7 training accuracy: 0.43081632653061225
At round 7 training loss: 0.40124202200344633
gradient difference: 0.0020331950371195984
At round 7 model param: 
[array([[-2.06077535],
       [ 1.04876958],
       [ 0.75000489],
       [ 0.37368251],
       [ 0.16251932],
       [ 0.0911856 ],
       [-0.00281718],
       [-0.0078513 ]]), 0.11140266539795059]
At round 8 accuracy: 0.44
At round 8 training accuracy: 0.43081632653061225
At round 8 training loss: 0.40124433806964327
gradient difference: 0.0020276653785423226
At round 8 model param: 
[array([[-2.05649665],
       [ 1.05445998],
       [ 0.75406653],
       [ 0.37853279],
       [ 0.16900953],
       [ 0.09412834],
       [-0.00325402],
       [-0.00501823]]), 0.11521164966481072]
At round 9 accuracy: 0.44
At round 9 training accuracy: 0.43081632653061225
At round 9 training loss: 0.4012859548841204
gradient difference: 0.0020649676872955265
At round 9 model param: 
[array([[-2.06584556],
       [ 1.04501172],
       [ 0.74641786],
       [ 0.3707636 ],
       [ 0.16299331],
       [ 0.08547468],
       [-0.00787504],
       [-0.01095908]]), 0.10892158640282494]
Total training time: 57.031965017318726s
At round 10 accuracy: 0.44
At round 10 training accuracy: 0.43081632653061225
At round 10 model param: 
[array([[-2.06584556],
       [ 1.04501172],
       [ 0.74641786],
       [ 0.3707636 ],
       [ 0.16299331],
       [ 0.08547468],
       [-0.00787504],
       [-0.01095908]]), 0.10892158640282494]
