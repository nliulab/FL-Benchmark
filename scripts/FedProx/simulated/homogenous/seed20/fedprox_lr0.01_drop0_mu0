Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 20
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.4319047619047619
At round 0 training accuracy: 0.4393877551020408
At round 0 training loss: 0.6931464076042175
gradient difference: 0.001001193263383768
At round 0 model param: 
[array([[-1.82343924],
       [ 0.87597338],
       [ 0.67336896],
       [ 0.3476315 ],
       [ 0.15991173],
       [ 0.10390499],
       [ 0.01496909],
       [ 0.0500775 ]]), 0.0625378645490855]
At round 1 accuracy: 0.4319047619047619
At round 1 training accuracy: 0.4393877551020408
At round 1 training loss: 0.40847996728760855
gradient difference: 0.0003401663137377929
At round 1 model param: 
[array([[-2.01571245],
       [ 0.97623464],
       [ 0.74876454],
       [ 0.38635335],
       [ 0.18582314],
       [ 0.11027453],
       [ 0.00924966],
       [ 0.06502748]]), 0.07639086619019508]
At round 2 accuracy: 0.4319047619047619
At round 2 training accuracy: 0.4393877551020408
At round 2 training loss: 0.40651352064950125
gradient difference: 0.00034212856937719787
At round 2 model param: 
[array([[-2.06374281e+00],
       [ 9.92521031e-01],
       [ 7.56667657e-01],
       [ 3.88853137e-01],
       [ 1.83945571e-01],
       [ 1.05943188e-01],
       [ 1.56228191e-03],
       [ 5.86552418e-02]]), 0.07430867478251457]
At round 3 accuracy: 0.4319047619047619
At round 3 training accuracy: 0.4393877551020408
At round 3 training loss: 0.4065276086330414
gradient difference: 0.0003468709080391746
At round 3 model param: 
[array([[-2.06499423],
       [ 1.00545161],
       [ 0.76741941],
       [ 0.39411649],
       [ 0.19123138],
       [ 0.1163192 ],
       [ 0.00625562],
       [ 0.06445015]]), 0.0824907368847302]
At round 4 accuracy: 0.4319047619047619
At round 4 training accuracy: 0.4393877551020408
At round 4 training loss: 0.40644846643720356
gradient difference: 0.0003401700761700641
At round 4 model param: 
[array([[-2.06589116],
       [ 1.00533976],
       [ 0.77008522],
       [ 0.39553778],
       [ 0.19137773],
       [ 0.12051279],
       [ 0.00903294],
       [ 0.06824026]]), 0.08513946511915751]
At round 5 accuracy: 0.4319047619047619
At round 5 training accuracy: 0.4393877551020408
At round 5 training loss: 0.40652274659701754
gradient difference: 0.0003394533187094529
At round 5 model param: 
[array([[-2.07133906],
       [ 1.0038086 ],
       [ 0.76316967],
       [ 0.39425614],
       [ 0.18803766],
       [ 0.11355167],
       [ 0.00468904],
       [ 0.06602208]]), 0.08213891301836286]
At round 6 accuracy: 0.4319047619047619
At round 6 training accuracy: 0.4393877551020408
At round 6 training loss: 0.4064096084662846
gradient difference: 0.00034135472215844745
At round 6 model param: 
[array([[-2.07154012],
       [ 1.00176632],
       [ 0.76269454],
       [ 0.39515732],
       [ 0.18756545],
       [ 0.11178427],
       [ 0.00766204],
       [ 0.06399222]]), 0.08223745971918106]
At round 7 accuracy: 0.4319047619047619
At round 7 training accuracy: 0.4393877551020408
At round 7 training loss: 0.4064111241272518
gradient difference: 0.0003433515485040488
At round 7 model param: 
[array([[-2.0740244 ],
       [ 0.99981383],
       [ 0.76441322],
       [ 0.39407204],
       [ 0.18525278],
       [ 0.10949751],
       [ 0.00248352],
       [ 0.0656191 ]]), 0.08133337859596525]
At round 8 accuracy: 0.4319047619047619
At round 8 training accuracy: 0.4393877551020408
At round 8 training loss: 0.40642116750989643
gradient difference: 0.00034411099237302963
At round 8 model param: 
[array([[-2.06268355],
       [ 1.00849179],
       [ 0.77399548],
       [ 0.39838699],
       [ 0.19587699],
       [ 0.11907398],
       [ 0.01066684],
       [ 0.07162903]]), 0.08878356963396072]
At round 9 accuracy: 0.4319047619047619
At round 9 training accuracy: 0.4393877551020408
At round 9 training loss: 0.4067057456289019
gradient difference: 0.00033867199149440304
At round 9 model param: 
[array([[-2.06552199],
       [ 1.00698912],
       [ 0.77050226],
       [ 0.39793839],
       [ 0.19109083],
       [ 0.11662513],
       [ 0.00819299],
       [ 0.06590214]]), 0.08573630026408605]
Total training time: 49.84496092796326s
At round 10 accuracy: 0.4319047619047619
At round 10 training accuracy: 0.4393877551020408
At round 10 model param: 
[array([[-2.06552199],
       [ 1.00698912],
       [ 0.77050226],
       [ 0.39793839],
       [ 0.19109083],
       [ 0.11662513],
       [ 0.00819299],
       [ 0.06590214]]), 0.08573630026408605]
