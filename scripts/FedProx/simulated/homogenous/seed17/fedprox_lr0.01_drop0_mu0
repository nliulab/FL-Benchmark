Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 17
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.4061904761904762
At round 0 training accuracy: 0.42346938775510207
At round 0 training loss: 0.6931464076042175
gradient difference: 0.004574037068571647
At round 0 model param: 
[array([[-1.69521691],
       [ 0.87750688],
       [ 0.75252913],
       [ 0.33757277],
       [ 0.16937971],
       [ 0.0552097 ],
       [-0.04230715],
       [-0.03682273]]), 0.12698971905878612]
At round 1 accuracy: 0.4061904761904762
At round 1 training accuracy: 0.42346938775510207
At round 1 training loss: 0.4177617004939488
gradient difference: 0.000704546561438249
At round 1 model param: 
[array([[-1.87098258],
       [ 0.96029142],
       [ 0.81925455],
       [ 0.36299847],
       [ 0.17727721],
       [ 0.05003053],
       [-0.05530858],
       [-0.03859572]]), 0.16774226512227738]
At round 2 accuracy: 0.4061904761904762
At round 2 training accuracy: 0.42346938775510207
At round 2 training loss: 0.41606639112745014
gradient difference: 0.0006807633513365337
At round 2 model param: 
[array([[-1.9150611 ],
       [ 0.96786111],
       [ 0.82438528],
       [ 0.36269293],
       [ 0.16796731],
       [ 0.04229698],
       [-0.06877425],
       [-0.04811521]]), 0.18396117218903132]
At round 3 accuracy: 0.4061904761904762
At round 3 training accuracy: 0.42346938775510207
At round 3 training loss: 0.4160492335047041
gradient difference: 0.0006760825157791323
At round 3 model param: 
[array([[-1.91777372],
       [ 0.97557923],
       [ 0.8283492 ],
       [ 0.36489303],
       [ 0.17375682],
       [ 0.04382501],
       [-0.06295015],
       [-0.04691848]]), 0.20106891223362514]
At round 4 accuracy: 0.4061904761904762
At round 4 training accuracy: 0.42346938775510207
At round 4 training loss: 0.41592209679739817
gradient difference: 0.0006765446752528267
At round 4 model param: 
[array([[-1.91423603],
       [ 0.97936707],
       [ 0.83329873],
       [ 0.37294512],
       [ 0.1770394 ],
       [ 0.04557634],
       [-0.06054726],
       [-0.04472431]]), 0.21243163517543248]
At round 5 accuracy: 0.4061904761904762
At round 5 training accuracy: 0.42346938775510207
At round 5 training loss: 0.4160440521580832
gradient difference: 0.0006804320735752182
At round 5 model param: 
[array([[-1.92581858],
       [ 0.96910872],
       [ 0.82589511],
       [ 0.36325031],
       [ 0.1687462 ],
       [ 0.03892151],
       [-0.06882954],
       [-0.05466149]]), 0.2097770039524351]
At round 6 accuracy: 0.4061904761904762
At round 6 training accuracy: 0.42346938775510207
At round 6 training loss: 0.41600295901298523
gradient difference: 0.0006753337872363357
At round 6 model param: 
[array([[-1.92023824],
       [ 0.97511779],
       [ 0.8299004 ],
       [ 0.36699806],
       [ 0.17250797],
       [ 0.04495491],
       [-0.06466742],
       [-0.05032817]]), 0.21652176124708994]
At round 7 accuracy: 0.4061904761904762
At round 7 training accuracy: 0.42346938775510207
At round 7 training loss: 0.4159247364316668
gradient difference: 0.0006769676144577574
At round 7 model param: 
[array([[-1.92463544],
       [ 0.97273396],
       [ 0.8239329 ],
       [ 0.36258445],
       [ 0.16873875],
       [ 0.04011532],
       [-0.06737686],
       [-0.04991686]]), 0.21449263393878937]
At round 8 accuracy: 0.4061904761904762
At round 8 training accuracy: 0.42346938775510207
At round 8 training loss: 0.41594715629305157
gradient difference: 0.0006749771883913054
At round 8 model param: 
[array([[-1.9178791 ],
       [ 0.9765998 ],
       [ 0.82912574],
       [ 0.36722603],
       [ 0.17589455],
       [ 0.04532704],
       [-0.06073609],
       [-0.04509897]]), 0.22093345437731063]
At round 9 accuracy: 0.4061904761904762
At round 9 training accuracy: 0.42346938775510207
At round 9 training loss: 0.4159861036709377
gradient difference: 0.0006776695681721348
At round 9 model param: 
[array([[-1.92564537],
       [ 0.97269738],
       [ 0.82369223],
       [ 0.3615678 ],
       [ 0.16796061],
       [ 0.03866556],
       [-0.07178585],
       [-0.05120826]]), 0.2157251856156758]
Total training time: 44.44476819038391s
At round 10 accuracy: 0.4061904761904762
At round 10 training accuracy: 0.42346938775510207
At round 10 model param: 
[array([[-1.92564537],
       [ 0.97269738],
       [ 0.82369223],
       [ 0.3615678 ],
       [ 0.16796061],
       [ 0.03866556],
       [-0.07178585],
       [-0.05120826]]), 0.2157251856156758]
