Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 16
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.4319047619047619
At round 0 training accuracy: 0.42959183673469387
At round 0 training loss: 0.6931464076042175
gradient difference: 0.002059222700794564
At round 0 model param: 
[array([[-1.78922491],
       [ 0.95895464],
       [ 0.6707572 ],
       [ 0.33512619],
       [ 0.19344833],
       [ 0.13360896],
       [-0.02185598],
       [-0.03642651]]), 0.07927826579127993]
At round 1 accuracy: 0.4319047619047619
At round 1 training accuracy: 0.42959183673469387
At round 1 training loss: 0.39807455454553875
gradient difference: 0.001752816903263314
At round 1 model param: 
[array([[-1.98570379],
       [ 1.04508193],
       [ 0.7393914 ],
       [ 0.36347432],
       [ 0.19804315],
       [ 0.1344335 ],
       [-0.03230522],
       [-0.0396643 ]]), 0.09817422926425934]
At round 2 accuracy: 0.4319047619047619
At round 2 training accuracy: 0.42959183673469387
At round 2 training loss: 0.39615560855184284
gradient difference: 0.0017383945897665115
At round 2 model param: 
[array([[-2.0218902 ],
       [ 1.06654052],
       [ 0.75751305],
       [ 0.36975245],
       [ 0.20382797],
       [ 0.13424385],
       [-0.03564621],
       [-0.03892596]]), 0.11281653706516538]
At round 3 accuracy: 0.4319047619047619
At round 3 training accuracy: 0.42959183673469387
At round 3 training loss: 0.39608062165124075
gradient difference: 0.0017392808756935378
At round 3 model param: 
[array([[-2.03378357],
       [ 1.06903609],
       [ 0.75568884],
       [ 0.36587216],
       [ 0.19897103],
       [ 0.13123174],
       [-0.0395342 ],
       [-0.04092559]]), 0.11762381983654839]
At round 4 accuracy: 0.4319047619047619
At round 4 training accuracy: 0.42959183673469387
At round 4 training loss: 0.396067989724023
gradient difference: 0.0017360840228645782
At round 4 model param: 
[array([[-2.03606057],
       [ 1.06943239],
       [ 0.75698727],
       [ 0.36603352],
       [ 0.1988887 ],
       [ 0.13238949],
       [-0.0379077 ],
       [-0.04253649]]), 0.12292027260575976]
At round 5 accuracy: 0.4319047619047619
At round 5 training accuracy: 0.42959183673469387
At round 5 training loss: 0.3960611139025007
gradient difference: 0.001734665785134859
At round 5 model param: 
[array([[-2.03996134],
       [ 1.06674086],
       [ 0.75325978],
       [ 0.36452968],
       [ 0.19876628],
       [ 0.1306657 ],
       [-0.04273435],
       [-0.04654407]]), 0.12297960264342171]
At round 6 accuracy: 0.4319047619047619
At round 6 training accuracy: 0.42959183673469387
At round 6 training loss: 0.39611739771706717
gradient difference: 0.0017363880870775925
At round 6 model param: 
[array([[-2.02829705],
       [ 1.07549806],
       [ 0.76517387],
       [ 0.37248651],
       [ 0.20853692],
       [ 0.13903087],
       [-0.03124472],
       [-0.03224897]]), 0.13410229342324392]
At round 7 accuracy: 0.4319047619047619
At round 7 training accuracy: 0.42959183673469387
At round 7 training loss: 0.39636214290346417
gradient difference: 0.0017365688626602754
At round 7 model param: 
[array([[-2.03959846],
       [ 1.06514827],
       [ 0.75581462],
       [ 0.36461553],
       [ 0.20228056],
       [ 0.12928755],
       [-0.03845439],
       [-0.04343545]]), 0.12765478874955857]
At round 8 accuracy: 0.4319047619047619
At round 8 training accuracy: 0.42959183673469387
At round 8 training loss: 0.3960681770529066
gradient difference: 0.0017351559449905395
At round 8 model param: 
[array([[-2.04019206],
       [ 1.06707546],
       [ 0.75583499],
       [ 0.36341768],
       [ 0.20118819],
       [ 0.12647499],
       [-0.03961115],
       [-0.04556626]]), 0.12742789089679718]
At round 9 accuracy: 0.4319047619047619
At round 9 training accuracy: 0.42959183673469387
At round 9 training loss: 0.3960865480559213
gradient difference: 0.0017365028585721093
At round 9 model param: 
[array([[-2.04292383],
       [ 1.06264762],
       [ 0.75355875],
       [ 0.35998412],
       [ 0.1962686 ],
       [ 0.12503632],
       [-0.04531155],
       [-0.04731517]]), 0.12495217791625432]
Total training time: 43.23046398162842s
At round 10 accuracy: 0.4319047619047619
At round 10 training accuracy: 0.42959183673469387
At round 10 model param: 
[array([[-2.04292383],
       [ 1.06264762],
       [ 0.75355875],
       [ 0.35998412],
       [ 0.1962686 ],
       [ 0.12503632],
       [-0.04531155],
       [-0.04731517]]), 0.12495217791625432]
