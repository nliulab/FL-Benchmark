Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 24
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.42952380952380953
At round 0 training accuracy: 0.4418367346938776
At round 0 training loss: 0.6931464076042175
gradient difference: 0.003948932703257514
At round 0 model param: 
[array([[-1.78606331],
       [ 0.84949647],
       [ 0.72096346],
       [ 0.28934777],
       [ 0.19799161],
       [ 0.08468224],
       [-0.00481635],
       [-0.03249143]]), 0.07751292821818165]
At round 1 accuracy: 0.42952380952380953
At round 1 training accuracy: 0.4418367346938776
At round 1 training loss: 0.4161337912082672
gradient difference: 0.002356163986566549
At round 1 model param: 
[array([[-1.97776817],
       [ 0.92584538],
       [ 0.78817161],
       [ 0.31585179],
       [ 0.21311679],
       [ 0.09355736],
       [-0.00544945],
       [-0.03675948]]), 0.11250118166208267]
At round 2 accuracy: 0.42952380952380953
At round 2 training accuracy: 0.4418367346938776
At round 2 training loss: 0.4146134683064052
gradient difference: 0.002387387793947293
At round 2 model param: 
[array([[-2.0119291 ],
       [ 0.94879779],
       [ 0.80182272],
       [ 0.32150963],
       [ 0.22180408],
       [ 0.09561773],
       [-0.00452861],
       [-0.03603401]]), 0.13792293359126365]
At round 3 accuracy: 0.42952380952380953
At round 3 training accuracy: 0.4418367346938776
At round 3 training loss: 0.41467817766325815
gradient difference: 0.002404313883702064
At round 3 model param: 
[array([[-2.02560084],
       [ 0.94447451],
       [ 0.79994593],
       [ 0.32129798],
       [ 0.21473198],
       [ 0.09223764],
       [-0.00924688],
       [-0.04091776]]), 0.14731927109616144]
At round 4 accuracy: 0.42952380952380953
At round 4 training accuracy: 0.4418367346938776
At round 4 training loss: 0.4145874423640115
gradient difference: 0.002407261762346783
At round 4 model param: 
[array([[-2.0234689 ],
       [ 0.95039827],
       [ 0.80323938],
       [ 0.3204279 ],
       [ 0.22014747],
       [ 0.09643862],
       [-0.00256973],
       [-0.03718206]]), 0.1589013316801616]
At round 5 accuracy: 0.42952380952380953
At round 5 training accuracy: 0.4418367346938776
At round 5 training loss: 0.4147360452583858
gradient difference: 0.0024139507240495822
At round 5 model param: 
[array([[-2.030003  ],
       [ 0.94387806],
       [ 0.79980826],
       [ 0.3177601 ],
       [ 0.21447348],
       [ 0.09143551],
       [-0.01140348],
       [-0.04506875]]), 0.15830466896295547]
At round 6 accuracy: 0.42952380952380953
At round 6 training accuracy: 0.4418367346938776
At round 6 training loss: 0.41459411808422636
gradient difference: 0.0024114027863388434
At round 6 model param: 
[array([[-2.02902688],
       [ 0.94416148],
       [ 0.80283777],
       [ 0.31833202],
       [ 0.21418563],
       [ 0.09763522],
       [-0.01212286],
       [-0.04444849]]), 0.16130247286387853]
At round 7 accuracy: 0.42952380952380953
At round 7 training accuracy: 0.4418367346938776
At round 7 training loss: 0.41460698417254854
gradient difference: 0.0024181451027945676
At round 7 model param: 
[array([[-2.02535585],
       [ 0.94772796],
       [ 0.80388886],
       [ 0.32076193],
       [ 0.21849771],
       [ 0.09858241],
       [-0.00886961],
       [-0.03935557]]), 0.1658580611859049]
At round 8 accuracy: 0.42952380952380953
At round 8 training accuracy: 0.4418367346938776
At round 8 training loss: 0.4147073669092996
gradient difference: 0.0024229254085039093
At round 8 model param: 
[array([[-2.02785063],
       [ 0.94665633],
       [ 0.80281261],
       [ 0.32081725],
       [ 0.21917852],
       [ 0.09689758],
       [-0.00951012],
       [-0.03984663]]), 0.16617323458194733]
At round 9 accuracy: 0.42952380952380953
At round 9 training accuracy: 0.4418367346938776
At round 9 training loss: 0.41467309849602835
gradient difference: 0.0024222086627819907
At round 9 model param: 
[array([[-2.03513002],
       [ 0.9407583 ],
       [ 0.79503577],
       [ 0.31382258],
       [ 0.21069208],
       [ 0.08946917],
       [-0.01540608],
       [-0.04666848]]), 0.16020002428974425]
Total training time: 46.58139514923096s
At round 10 accuracy: 0.42952380952380953
At round 10 training accuracy: 0.4418367346938776
At round 10 model param: 
[array([[-2.03513002],
       [ 0.9407583 ],
       [ 0.79503577],
       [ 0.31382258],
       [ 0.21069208],
       [ 0.08946917],
       [-0.01540608],
       [-0.04666848]]), 0.16020002428974425]
