Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 7
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.43857142857142856
At round 0 training accuracy: 0.4363265306122449
At round 0 training loss: 0.6931464076042175
gradient difference: 0.002211536601275582
At round 0 model param: 
[array([[-1.79427721],
       [ 0.84760624],
       [ 0.68778749],
       [ 0.38864722],
       [ 0.23322938],
       [ 0.10810165],
       [-0.03661308],
       [ 0.04741916]]), -0.01760540317211832]
At round 1 accuracy: 0.43857142857142856
At round 1 training accuracy: 0.4363265306122449
At round 1 training loss: 0.4145028122833797
gradient difference: 0.0017010969144553897
At round 1 model param: 
[array([[-1.97176283],
       [ 0.9405989 ],
       [ 0.75161891],
       [ 0.42193303],
       [ 0.24974633],
       [ 0.11324117],
       [-0.04401303],
       [ 0.04756561]]), -0.04636315682104656]
At round 2 accuracy: 0.43857142857142856
At round 2 training accuracy: 0.4363265306122449
At round 2 training loss: 0.413192161491939
gradient difference: 0.0017226406312612415
At round 2 model param: 
[array([[-2.00840529],
       [ 0.95253689],
       [ 0.76565383],
       [ 0.4280102 ],
       [ 0.2545912 ],
       [ 0.1164389 ],
       [-0.04845469],
       [ 0.05056772]]), -0.06363048231495279]
At round 3 accuracy: 0.43857142857142856
At round 3 training accuracy: 0.4363265306122449
At round 3 training loss: 0.41326047692980084
gradient difference: 0.0017318362382445843
At round 3 model param: 
[array([[-2.00243189],
       [ 0.96772601],
       [ 0.77920175],
       [ 0.4366892 ],
       [ 0.26531212],
       [ 0.12577469],
       [-0.03815291],
       [ 0.05936921]]), -0.06462663929310761]
At round 4 accuracy: 0.43857142857142856
At round 4 training accuracy: 0.4363265306122449
At round 4 training loss: 0.4134068361350468
gradient difference: 0.0017520735058088963
At round 4 model param: 
[array([[-2.006843  ],
       [ 0.96705266],
       [ 0.77653549],
       [ 0.43520981],
       [ 0.26110121],
       [ 0.1206362 ],
       [-0.03858075],
       [ 0.059018  ]]), -0.0733690546559436]
At round 5 accuracy: 0.43857142857142856
At round 5 training accuracy: 0.4363265306122449
At round 5 training loss: 0.4132557085582188
gradient difference: 0.0017444469988477155
At round 5 model param: 
[array([[-2.01027911],
       [ 0.96183176],
       [ 0.77381844],
       [ 0.43603199],
       [ 0.26080814],
       [ 0.12352026],
       [-0.04322556],
       [ 0.05921057]]), -0.07802821363189391]
At round 6 accuracy: 0.43857142857142856
At round 6 training accuracy: 0.4363265306122449
At round 6 training loss: 0.41322465028081623
gradient difference: 0.0017454704236768922
At round 6 model param: 
[array([[-2.01118227],
       [ 0.96134756],
       [ 0.77579581],
       [ 0.43436628],
       [ 0.26014641],
       [ 0.12234358],
       [-0.04370281],
       [ 0.056721  ]]), -0.08082987048796245]
At round 7 accuracy: 0.43857142857142856
At round 7 training accuracy: 0.4363265306122449
At round 7 training loss: 0.4132236838340759
gradient difference: 0.0017433185383159582
At round 7 model param: 
[array([[-2.00948633],
       [ 0.96429389],
       [ 0.77488324],
       [ 0.43727719],
       [ 0.25959873],
       [ 0.11753408],
       [-0.04611643],
       [ 0.05638114]]), -0.08192751583244119]
At round 8 accuracy: 0.43857142857142856
At round 8 training accuracy: 0.4363265306122449
At round 8 training loss: 0.41321692296436857
gradient difference: 0.0017384991573379013
At round 8 model param: 
[array([[-2.01159854],
       [ 0.96364236],
       [ 0.77412502],
       [ 0.433542  ],
       [ 0.26083101],
       [ 0.12031557],
       [-0.04718462],
       [ 0.05447757]]), -0.08338957546012742]
At round 9 accuracy: 0.43857142857142856
At round 9 training accuracy: 0.4363265306122449
At round 9 training loss: 0.413226808820452
gradient difference: 0.0017378557849589305
At round 9 model param: 
[array([[-2.00598979],
       [ 0.96783589],
       [ 0.77696381],
       [ 0.43686148],
       [ 0.26296225],
       [ 0.12179259],
       [-0.04164161],
       [ 0.05994171]]), -0.08041804869260107]
Total training time: 43.11382079124451s
At round 10 accuracy: 0.43857142857142856
At round 10 training accuracy: 0.4363265306122449
At round 10 model param: 
[array([[-2.00598979],
       [ 0.96783589],
       [ 0.77696381],
       [ 0.43686148],
       [ 0.26296225],
       [ 0.12179259],
       [-0.04164161],
       [ 0.05994171]]), -0.08041804869260107]
