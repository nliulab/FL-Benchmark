Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 5
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.43952380952380954
At round 0 training accuracy: 0.4293877551020408
At round 0 training loss: 0.6931464076042175
gradient difference: 0.003758747823998952
At round 0 model param: 
[array([[-1.80146129],
       [ 0.87097668],
       [ 0.7167446 ],
       [ 0.39198806],
       [ 0.15137931],
       [ 0.18002891],
       [ 0.0111045 ],
       [-0.03411118]]), 0.04587174419845853]
At round 1 accuracy: 0.43952380952380954
At round 1 training accuracy: 0.4293877551020408
At round 1 training loss: 0.40422294821058
gradient difference: 0.0011637316764973444
At round 1 model param: 
[array([[-1.99407114],
       [ 0.96230089],
       [ 0.77992581],
       [ 0.4140607 ],
       [ 0.15620859],
       [ 0.18451581],
       [ 0.00383859],
       [-0.05125208]]), 0.035981000534125736]
At round 2 accuracy: 0.43952380952380954
At round 2 training accuracy: 0.4293877551020408
At round 2 training loss: 0.40228885412216187
gradient difference: 0.0010753638011697685
At round 2 model param: 
[array([[-2.03343686e+00],
       [ 9.82220956e-01],
       [ 7.95341390e-01],
       [ 4.21273657e-01],
       [ 1.56327507e-01],
       [ 1.88342105e-01],
       [ 2.01726306e-03],
       [-5.66494066e-02]]), 0.03295476255672319]
At round 3 accuracy: 0.43952380952380954
At round 3 training accuracy: 0.4293877551020408
At round 3 training loss: 0.402260171515601
gradient difference: 0.001060487520625071
At round 3 model param: 
[array([[-2.04335650e+00],
       [ 9.84367251e-01],
       [ 7.97981364e-01],
       [ 4.21002797e-01],
       [ 1.53560466e-01],
       [ 1.86639177e-01],
       [-8.70283427e-04],
       [-5.39862380e-02]]), 0.029691622193370546]
At round 4 accuracy: 0.43952380952380954
At round 4 training accuracy: 0.4293877551020408
At round 4 training loss: 0.40231520363262724
gradient difference: 0.001054569959120426
At round 4 model param: 
[array([[-2.03424394],
       [ 0.99493412],
       [ 0.8071855 ],
       [ 0.43018193],
       [ 0.16219247],
       [ 0.19328733],
       [ 0.00743223],
       [-0.04752759]]), 0.03677781458411898]
At round 5 accuracy: 0.43952380952380954
At round 5 training accuracy: 0.4293877551020408
At round 5 training loss: 0.4023512474128178
gradient difference: 0.0010561618773298406
At round 5 model param: 
[array([[-2.03651941],
       [ 0.99179826],
       [ 0.80759028],
       [ 0.42504803],
       [ 0.16391421],
       [ 0.19362489],
       [ 0.00883134],
       [-0.04657226]]), 0.035300864705017636]
At round 6 accuracy: 0.43952380952380954
At round 6 training accuracy: 0.4293877551020408
At round 6 training loss: 0.4023132451942989
gradient difference: 0.0010582799261439548
At round 6 model param: 
[array([[-2.0403227 ],
       [ 0.98966799],
       [ 0.8049339 ],
       [ 0.42515885],
       [ 0.16038151],
       [ 0.18844662],
       [ 0.0051146 ],
       [-0.05024906]]), 0.03157811079706464]
At round 7 accuracy: 0.43952380952380954
At round 7 training accuracy: 0.4293877551020408
At round 7 training loss: 0.402235746383667
gradient difference: 0.001054480415127728
At round 7 model param: 
[array([[-2.04322301],
       [ 0.9875686 ],
       [ 0.80098041],
       [ 0.42072534],
       [ 0.15558245],
       [ 0.18864873],
       [ 0.00289424],
       [-0.05588649]]), 0.028519882155316218]
At round 8 accuracy: 0.43952380952380954
At round 8 training accuracy: 0.4293877551020408
At round 8 training loss: 0.40227411474500385
gradient difference: 0.0010570253753444156
At round 8 model param: 
[array([[-2.03633719],
       [ 0.99332856],
       [ 0.80828024],
       [ 0.42645187],
       [ 0.16008302],
       [ 0.19093616],
       [ 0.00930419],
       [-0.04580226]]), 0.033238722277539115]
At round 9 accuracy: 0.43952380952380954
At round 9 training accuracy: 0.4293877551020408
At round 9 training loss: 0.4022979268005916
gradient difference: 0.0010541647083190356
At round 9 model param: 
[array([[-2.03997069],
       [ 0.99074578],
       [ 0.80316009],
       [ 0.42586064],
       [ 0.1604385 ],
       [ 0.18946637],
       [ 0.00385578],
       [-0.05030717]]), 0.03092086740902492]
Total training time: 43.558823108673096s
At round 10 accuracy: 0.43952380952380954
At round 10 training accuracy: 0.4293877551020408
At round 10 model param: 
[array([[-2.03997069],
       [ 0.99074578],
       [ 0.80316009],
       [ 0.42586064],
       [ 0.1604385 ],
       [ 0.18946637],
       [ 0.00385578],
       [-0.05030717]]), 0.03092086740902492]
