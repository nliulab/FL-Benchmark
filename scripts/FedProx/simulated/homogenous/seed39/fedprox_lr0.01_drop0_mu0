Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 39
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.44333333333333336
At round 0 training accuracy: 0.4330612244897959
At round 0 training loss: 0.6931464076042175
gradient difference: 0.0010134924942983377
At round 0 model param: 
[array([[-1.80707509],
       [ 0.87870412],
       [ 0.74756978],
       [ 0.34470904],
       [ 0.20285959],
       [ 0.15254994],
       [-0.02015716],
       [ 0.03020512]]), -0.041425827740957696]
At round 1 accuracy: 0.44333333333333336
At round 1 training accuracy: 0.4330612244897959
At round 1 training loss: 0.40419441035815645
gradient difference: 0.00033659656322444846
At round 1 model param: 
[array([[-2.00349469],
       [ 0.96876342],
       [ 0.82034933],
       [ 0.38569475],
       [ 0.21862174],
       [ 0.16014711],
       [-0.02783948],
       [ 0.02785654]]), -0.09649989008903503]
At round 2 accuracy: 0.44333333333333336
At round 2 training accuracy: 0.4330612244897959
At round 2 training loss: 0.40228759816714693
gradient difference: 0.00032830472497761724
At round 2 model param: 
[array([[-2.03648935],
       [ 0.99440236],
       [ 0.8451827 ],
       [ 0.40089539],
       [ 0.23314088],
       [ 0.17180963],
       [-0.01955586],
       [ 0.03823272]]), -0.12248045525380544]
At round 3 accuracy: 0.44333333333333336
At round 3 training accuracy: 0.4330612244897959
At round 3 training loss: 0.40224963426589966
gradient difference: 0.00032613236393525593
At round 3 model param: 
[array([[-2.04677916],
       [ 1.00012657],
       [ 0.84771045],
       [ 0.40349784],
       [ 0.23260761],
       [ 0.17609815],
       [-0.01769468],
       [ 0.04131672]]), -0.14130585959979466]
At round 4 accuracy: 0.44333333333333336
At round 4 training accuracy: 0.4330612244897959
At round 4 training loss: 0.40221256017684937
gradient difference: 0.0003245122089951513
At round 4 model param: 
[array([[-2.04740555],
       [ 1.00240599],
       [ 0.84925754],
       [ 0.40659285],
       [ 0.23345772],
       [ 0.17326434],
       [-0.02345927],
       [ 0.04220523]]), -0.15356036382062094]
At round 5 accuracy: 0.44333333333333336
At round 5 training accuracy: 0.4330612244897959
At round 5 training loss: 0.40219045536858694
gradient difference: 0.00032600308727678525
At round 5 model param: 
[array([[-2.05317031],
       [ 0.99801895],
       [ 0.84840458],
       [ 0.40148666],
       [ 0.22939298],
       [ 0.16778358],
       [-0.02819263],
       [ 0.03491905]]), -0.16574478362287795]
At round 6 accuracy: 0.44333333333333336
At round 6 training accuracy: 0.4330612244897959
At round 6 training loss: 0.40231049060821533
gradient difference: 0.00032567176423187427
At round 6 model param: 
[array([[-2.05122726],
       [ 1.00063195],
       [ 0.84860424],
       [ 0.40419136],
       [ 0.23511234],
       [ 0.1734302 ],
       [-0.02402169],
       [ 0.03904825]]), -0.16732880898884364]
At round 7 accuracy: 0.44333333333333336
At round 7 training accuracy: 0.4330612244897959
At round 7 training loss: 0.4022079961640494
gradient difference: 0.0003256251291133077
At round 7 model param: 
[array([[-2.04916167],
       [ 1.00235998],
       [ 0.84984349],
       [ 0.40832696],
       [ 0.23452068],
       [ 0.1739265 ],
       [-0.01929543],
       [ 0.04278856]]), -0.16767390923840658]
At round 8 accuracy: 0.44333333333333336
At round 8 training accuracy: 0.4330612244897959
At round 8 training loss: 0.4021881903920855
gradient difference: 0.00032399129991321393
At round 8 model param: 
[array([[-2.04415938],
       [ 1.00627902],
       [ 0.85491409],
       [ 0.40761693],
       [ 0.2370023 ],
       [ 0.17903614],
       [-0.01846066],
       [ 0.04189361]]), -0.16670416934149607]
At round 9 accuracy: 0.44333333333333336
At round 9 training accuracy: 0.4330612244897959
At round 9 training loss: 0.40223407319613863
gradient difference: 0.00032489205209692835
At round 9 model param: 
[array([[-2.04886491],
       [ 1.0022537 ],
       [ 0.84989731],
       [ 0.40411633],
       [ 0.2337282 ],
       [ 0.17363713],
       [-0.01983249],
       [ 0.04067961]]), -0.17092081904411316]
Total training time: 41.30627679824829s
At round 10 accuracy: 0.44333333333333336
At round 10 training accuracy: 0.4330612244897959
At round 10 model param: 
[array([[-2.04886491],
       [ 1.0022537 ],
       [ 0.84989731],
       [ 0.40411633],
       [ 0.2337282 ],
       [ 0.17363713],
       [-0.01983249],
       [ 0.04067961]]), -0.17092081904411316]
