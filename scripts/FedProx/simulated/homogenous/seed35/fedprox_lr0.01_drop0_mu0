Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 35
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.43714285714285717
At round 0 training accuracy: 0.43326530612244896
At round 0 training loss: 0.6931464076042175
gradient difference: 0.0022542489814812243
At round 0 model param: 
[array([[-1.7631573 ],
       [ 0.86001179],
       [ 0.70279393],
       [ 0.38131896],
       [ 0.15817443],
       [ 0.02860387],
       [-0.02997894],
       [-0.00877209]]), 0.1053679808974266]
At round 1 accuracy: 0.43714285714285717
At round 1 training accuracy: 0.43326530612244896
At round 1 training loss: 0.4223430497305734
gradient difference: 0.0016084276740381514
At round 1 model param: 
[array([[-1.95375563],
       [ 0.95136655],
       [ 0.77046594],
       [ 0.413524  ],
       [ 0.16654653],
       [ 0.03249547],
       [-0.03431122],
       [-0.01638253]]), 0.14427623684917176]
At round 2 accuracy: 0.43714285714285717
At round 2 training accuracy: 0.43326530612244896
At round 2 training loss: 0.4207320937088558
gradient difference: 0.0016368277559671365
At round 2 model param: 
[array([[-1.99429519],
       [ 0.96759596],
       [ 0.77961724],
       [ 0.42098766],
       [ 0.16881536],
       [ 0.03459213],
       [-0.0346013 ],
       [-0.01943509]]), 0.16705707779952458]
At round 3 accuracy: 0.43714285714285717
At round 3 training accuracy: 0.43326530612244896
At round 3 training loss: 0.4206665371145521
gradient difference: 0.0016457047419152105
At round 3 model param: 
[array([[-2.00733747],
       [ 0.96666478],
       [ 0.77971126],
       [ 0.41902624],
       [ 0.16248133],
       [ 0.02986704],
       [-0.03762852],
       [-0.02212407]]), 0.1779352171080453]
At round 4 accuracy: 0.43714285714285717
At round 4 training accuracy: 0.43326530612244896
At round 4 training loss: 0.4207679714475359
gradient difference: 0.0016407967004906045
At round 4 model param: 
[array([[-2.00099739],
       [ 0.97571542],
       [ 0.78526735],
       [ 0.42681413],
       [ 0.16963876],
       [ 0.03672791],
       [-0.03252428],
       [-0.01343947]]), 0.19310147208826883]
At round 5 accuracy: 0.43714285714285717
At round 5 training accuracy: 0.43326530612244896
At round 5 training loss: 0.42072538818631855
gradient difference: 0.0016577334255555773
At round 5 model param: 
[array([[-1.9966033 ],
       [ 0.97894267],
       [ 0.79437132],
       [ 0.42725273],
       [ 0.17308822],
       [ 0.04248782],
       [-0.02628869],
       [-0.01118438]]), 0.20202048335756576]
At round 6 accuracy: 0.43714285714285717
At round 6 training accuracy: 0.43326530612244896
At round 6 training loss: 0.42106673972947256
gradient difference: 0.0016605747004705656
At round 6 model param: 
[array([[-2.00299944],
       [ 0.97726052],
       [ 0.78434428],
       [ 0.42238492],
       [ 0.16932807],
       [ 0.03569934],
       [-0.03361862],
       [-0.01620147]]), 0.2005346076829093]
At round 7 accuracy: 0.43714285714285717
At round 7 training accuracy: 0.43326530612244896
At round 7 training loss: 0.4207080602645874
gradient difference: 0.001657072312790871
At round 7 model param: 
[array([[-2.01218845],
       [ 0.96698528],
       [ 0.77905761],
       [ 0.41524346],
       [ 0.15936679],
       [ 0.02497416],
       [-0.04196514],
       [-0.02932316]]), 0.19356174979891097]
At round 8 accuracy: 0.43714285714285717
At round 8 training accuracy: 0.43326530612244896
At round 8 training loss: 0.42087462544441223
gradient difference: 0.0016330202147896867
At round 8 model param: 
[array([[-2.01121739],
       [ 0.96986608],
       [ 0.77948586],
       [ 0.41481694],
       [ 0.1613931 ],
       [ 0.02899763],
       [-0.04212558],
       [-0.02520154]]), 0.19623727457863943]
At round 9 accuracy: 0.43714285714285717
At round 9 training accuracy: 0.43326530612244896
At round 9 training loss: 0.4207631605012076
gradient difference: 0.001638259808381475
At round 9 model param: 
[array([[-1.99799699],
       [ 0.97803159],
       [ 0.79152655],
       [ 0.43076491],
       [ 0.17676246],
       [ 0.0393597 ],
       [-0.03013583],
       [-0.01151356]]), 0.20827049016952515]
Total training time: 41.564034938812256s
At round 10 accuracy: 0.43714285714285717
At round 10 training accuracy: 0.43326530612244896
At round 10 model param: 
[array([[-1.99799699],
       [ 0.97803159],
       [ 0.79152655],
       [ 0.43076491],
       [ 0.17676246],
       [ 0.0393597 ],
       [-0.03013583],
       [-0.01151356]]), 0.20827049016952515]
