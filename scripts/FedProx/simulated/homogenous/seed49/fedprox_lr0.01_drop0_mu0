Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 49
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.43857142857142856
At round 0 training accuracy: 0.42428571428571427
At round 0 training loss: 0.6931464076042175
gradient difference: 0.002299604269321927
At round 0 model param: 
[array([[-1.71635629],
       [ 0.92489187],
       [ 0.71095736],
       [ 0.33966464],
       [ 0.18618968],
       [ 0.1265975 ],
       [ 0.07338037],
       [-0.01965537]]), -0.05488983115979603]
At round 1 accuracy: 0.43857142857142856
At round 1 training accuracy: 0.42428571428571427
At round 1 training loss: 0.41470164486340116
gradient difference: 0.002604959881253727
At round 1 model param: 
[array([[-1.89333745],
       [ 1.0160758 ],
       [ 0.77598742],
       [ 0.37574944],
       [ 0.20756567],
       [ 0.14050621],
       [ 0.07539539],
       [-0.02446108]]), -0.1300010663856353]
At round 2 accuracy: 0.43857142857142856
At round 2 training accuracy: 0.42428571428571427
At round 2 training loss: 0.4127770662307739
gradient difference: 0.0026694527453664757
At round 2 model param: 
[array([[-1.92222125],
       [ 1.04106518],
       [ 0.79350703],
       [ 0.38823289],
       [ 0.2175152 ],
       [ 0.15047574],
       [ 0.0781293 ],
       [-0.02285218]]), -0.1738713468824114]
At round 3 accuracy: 0.43857142857142856
At round 3 training accuracy: 0.42428571428571427
At round 3 training loss: 0.4127297444002969
gradient difference: 0.0027127504554374875
At round 3 model param: 
[array([[-1.92937183],
       [ 1.04466362],
       [ 0.80084294],
       [ 0.3899928 ],
       [ 0.21860582],
       [ 0.14826876],
       [ 0.08076298],
       [-0.02365582]]), -0.20299531093665532]
At round 4 accuracy: 0.43857142857142856
At round 4 training accuracy: 0.42428571428571427
At round 4 training loss: 0.41273770162037443
gradient difference: 0.0026946827058858453
At round 4 model param: 
[array([[-1.92984331],
       [ 1.0469096 ],
       [ 0.80036563],
       [ 0.39219743],
       [ 0.22080593],
       [ 0.15129229],
       [ 0.08139229],
       [-0.01855613]]), -0.2187045386859349]
At round 5 accuracy: 0.43857142857142856
At round 5 training accuracy: 0.42428571428571427
At round 5 training loss: 0.41274522457803997
gradient difference: 0.0027008006045063284
At round 5 model param: 
[array([[-1.93321565],
       [ 1.04821277],
       [ 0.79861975],
       [ 0.38771874],
       [ 0.21810839],
       [ 0.15011413],
       [ 0.07996325],
       [-0.0234911 ]]), -0.2318290365593774]
At round 6 accuracy: 0.43857142857142856
At round 6 training accuracy: 0.42428571428571427
At round 6 training loss: 0.41286358663014006
gradient difference: 0.002657983047276836
At round 6 model param: 
[array([[-1.92988947],
       [ 1.04808239],
       [ 0.80133861],
       [ 0.39157425],
       [ 0.22440333],
       [ 0.15463389],
       [ 0.08399862],
       [-0.02113377]]), -0.2345858727182661]
At round 7 accuracy: 0.43857142857142856
At round 7 training accuracy: 0.42428571428571427
At round 7 training loss: 0.4127715698310307
gradient difference: 0.002701062083403958
At round 7 model param: 
[array([[-1.9293178 ],
       [ 1.05047933],
       [ 0.80220187],
       [ 0.39166285],
       [ 0.22024236],
       [ 0.15254963],
       [ 0.08513757],
       [-0.02347203]]), -0.2385196089744568]
At round 8 accuracy: 0.43857142857142856
At round 8 training accuracy: 0.42428571428571427
At round 8 training loss: 0.41279742973191397
gradient difference: 0.0026854217551831733
At round 8 model param: 
[array([[-1.9244208 ],
       [ 1.05226239],
       [ 0.80539345],
       [ 0.40124183],
       [ 0.23017834],
       [ 0.15862611],
       [ 0.09109502],
       [-0.0145173 ]]), -0.23465481400489807]
At round 9 accuracy: 0.43857142857142856
At round 9 training accuracy: 0.42428571428571427
At round 9 training loss: 0.4128406729016985
gradient difference: 0.0027708090975049806
At round 9 model param: 
[array([[-1.92901886],
       [ 1.04887923],
       [ 0.8031707 ],
       [ 0.3938686 ],
       [ 0.22302095],
       [ 0.15296477],
       [ 0.08119444],
       [-0.01680644]]), -0.24115246321473802]
Total training time: 37.5268988609314s
At round 10 accuracy: 0.43857142857142856
At round 10 training accuracy: 0.42428571428571427
At round 10 model param: 
[array([[-1.92901886],
       [ 1.04887923],
       [ 0.8031707 ],
       [ 0.3938686 ],
       [ 0.22302095],
       [ 0.15296477],
       [ 0.08119444],
       [-0.01680644]]), -0.24115246321473802]
