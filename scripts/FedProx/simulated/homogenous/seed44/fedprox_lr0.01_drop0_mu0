Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 44
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.4257142857142857
At round 0 training accuracy: 0.43551020408163266
At round 0 training loss: 0.6931464076042175
gradient difference: 0.0021954819473367785
At round 0 model param: 
[array([[-1.74663424],
       [ 0.84406264],
       [ 0.75389858],
       [ 0.33438955],
       [ 0.19726357],
       [ 0.05830582],
       [ 0.02660341],
       [-0.02606073]]), 0.040246617048978806]
At round 1 accuracy: 0.4257142857142857
At round 1 training accuracy: 0.43551020408163266
At round 1 training loss: 0.41571103249277386
gradient difference: 0.0006709226914486981
At round 1 model param: 
[array([[-1.91705981],
       [ 0.93071035],
       [ 0.82315226],
       [ 0.37039418],
       [ 0.21320069],
       [ 0.06121587],
       [ 0.02783503],
       [-0.02314248]]), 0.04287896518196378]
At round 2 accuracy: 0.4257142857142857
At round 2 training accuracy: 0.43551020408163266
At round 2 training loss: 0.41438381161008564
gradient difference: 0.000643173553816333
At round 2 model param: 
[array([[-1.95020011],
       [ 0.94717804],
       [ 0.83521158],
       [ 0.37508053],
       [ 0.21370522],
       [ 0.06712228],
       [ 0.0284034 ],
       [-0.02497409]]), 0.04207271710038185]
At round 3 accuracy: 0.4257142857142857
At round 3 training accuracy: 0.43551020408163266
At round 3 training loss: 0.41434404253959656
gradient difference: 0.0006378508373275838
At round 3 model param: 
[array([[-1.95988945],
       [ 0.94913099],
       [ 0.83518032],
       [ 0.37632572],
       [ 0.21462069],
       [ 0.06135636],
       [ 0.02776937],
       [-0.02546425]]), 0.040564404002257755]
At round 4 accuracy: 0.4257142857142857
At round 4 training accuracy: 0.43551020408163266
At round 4 training loss: 0.4142981086458479
gradient difference: 0.0006291390836003604
At round 4 model param: 
[array([[-1.95678779],
       [ 0.9539117 ],
       [ 0.83919038],
       [ 0.37869519],
       [ 0.21995143],
       [ 0.06514445],
       [ 0.03115263],
       [-0.02202755]]), 0.04324442467519215]
At round 5 accuracy: 0.4257142857142857
At round 5 training accuracy: 0.43551020408163266
At round 5 training loss: 0.4144461027213505
gradient difference: 0.0006433833434991553
At round 5 model param: 
[array([[-1.95999735],
       [ 0.9521521 ],
       [ 0.83640597],
       [ 0.37634698],
       [ 0.21384274],
       [ 0.06269552],
       [ 0.03070833],
       [-0.02340012]]), 0.04107336540307317]
At round 6 accuracy: 0.4257142857142857
At round 6 training accuracy: 0.43551020408163266
At round 6 training loss: 0.4143261866910117
gradient difference: 0.000633108278347698
At round 6 model param: 
[array([[-1.95972821],
       [ 0.95010414],
       [ 0.83740852],
       [ 0.37786009],
       [ 0.2161184 ],
       [ 0.06502402],
       [ 0.03017108],
       [-0.02633571]]), 0.041053773037024906]
At round 7 accuracy: 0.4257142857142857
At round 7 training accuracy: 0.43551020408163266
At round 7 training loss: 0.41433486342430115
gradient difference: 0.0006342855732462575
At round 7 model param: 
[array([[-1.96512258],
       [ 0.94672081],
       [ 0.83103946],
       [ 0.37565411],
       [ 0.21044403],
       [ 0.05859359],
       [ 0.02595366],
       [-0.02769096]]), 0.037596135799373896]
At round 8 accuracy: 0.4257142857142857
At round 8 training accuracy: 0.43551020408163266
At round 8 training loss: 0.4142960565430777
gradient difference: 0.0006177468712211292
At round 8 model param: 
[array([[-1.9654288 ],
       [ 0.94580374],
       [ 0.83287135],
       [ 0.37121975],
       [ 0.20851503],
       [ 0.05823502],
       [ 0.02400253],
       [-0.02971615]]), 0.036115321197680066]
At round 9 accuracy: 0.4257142857142857
At round 9 training accuracy: 0.43551020408163266
At round 9 training loss: 0.41432545866285053
gradient difference: 0.0006133392899472721
At round 9 model param: 
[array([[-1.96328436],
       [ 0.94791759],
       [ 0.83367431],
       [ 0.3748323 ],
       [ 0.21624586],
       [ 0.06041809],
       [ 0.02566459],
       [-0.0286685 ]]), 0.03857648159776415]
Total training time: 35.38730502128601s
At round 10 accuracy: 0.4257142857142857
At round 10 training accuracy: 0.43551020408163266
At round 10 model param: 
[array([[-1.96328436],
       [ 0.94791759],
       [ 0.83367431],
       [ 0.3748323 ],
       [ 0.21624586],
       [ 0.06041809],
       [ 0.02566459],
       [-0.0286685 ]]), 0.03857648159776415]
