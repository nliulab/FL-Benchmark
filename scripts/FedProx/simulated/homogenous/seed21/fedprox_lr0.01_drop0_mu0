Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 21
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.42952380952380953
At round 0 training accuracy: 0.4440816326530612
At round 0 training loss: 0.6931464076042175
gradient difference: 0.0017764600168765834
At round 0 model param: 
[array([[-1.78170283],
       [ 0.88207195],
       [ 0.72454863],
       [ 0.36789734],
       [ 0.21371242],
       [ 0.07318752],
       [ 0.01305884],
       [-0.05636854]]), -0.026168697264178524]
At round 1 accuracy: 0.42952380952380953
At round 1 training accuracy: 0.4440816326530612
At round 1 training loss: 0.4126739033630916
gradient difference: 0.0009843799128804462
At round 1 model param: 
[array([[-1.97172965],
       [ 0.9828068 ],
       [ 0.80682253],
       [ 0.4084281 ],
       [ 0.24163564],
       [ 0.0842551 ],
       [ 0.01972205],
       [-0.05786236]]), -0.05962737862552915]
At round 2 accuracy: 0.42952380952380953
At round 2 training accuracy: 0.4440816326530612
At round 2 training loss: 0.41071812169892447
gradient difference: 0.0009844082907970457
At round 2 model param: 
[array([[-2.00907837],
       [ 1.00524184],
       [ 0.82670048],
       [ 0.41835072],
       [ 0.25179391],
       [ 0.08887453],
       [ 0.02245897],
       [-0.0554033 ]]), -0.08203735521861485]
At round 3 accuracy: 0.42952380952380953
At round 3 training accuracy: 0.4440816326530612
At round 3 training loss: 0.4106371062142508
gradient difference: 0.0009862976460120185
At round 3 model param: 
[array([[-2.02395201],
       [ 1.00368711],
       [ 0.8269256 ],
       [ 0.4138361 ],
       [ 0.24915307],
       [ 0.09117036],
       [ 0.02032972],
       [-0.05991838]]), -0.1000369233744485]
At round 4 accuracy: 0.42952380952380953
At round 4 training accuracy: 0.4440816326530612
At round 4 training loss: 0.4105749641145979
gradient difference: 0.000977050143520644
At round 4 model param: 
[array([[-2.02574556],
       [ 1.00563769],
       [ 0.82595163],
       [ 0.41851598],
       [ 0.24834229],
       [ 0.08848706],
       [ 0.02251012],
       [-0.05829233]]), -0.10953091191393989]
At round 5 accuracy: 0.42952380952380953
At round 5 training accuracy: 0.4440816326530612
At round 5 training loss: 0.41057898317064556
gradient difference: 0.000978460002833082
At round 5 model param: 
[array([[-2.02035621],
       [ 1.00932077],
       [ 0.83092256],
       [ 0.42338319],
       [ 0.25456454],
       [ 0.09149402],
       [ 0.02432453],
       [-0.05554837]]), -0.11163687386683055]
At round 6 accuracy: 0.42952380952380953
At round 6 training accuracy: 0.4440816326530612
At round 6 training loss: 0.4105528550488608
gradient difference: 0.000984153236739978
At round 6 model param: 
[array([[-2.02995069],
       [ 1.0024038 ],
       [ 0.82380812],
       [ 0.41323461],
       [ 0.2436533 ],
       [ 0.08379844],
       [ 0.01893169],
       [-0.06249461]]), -0.12202929705381393]
At round 7 accuracy: 0.42952380952380953
At round 7 training accuracy: 0.4440816326530612
At round 7 training loss: 0.4108496223177229
gradient difference: 0.0009727784698433055
At round 7 model param: 
[array([[-2.02016977],
       [ 1.01171309],
       [ 0.83183989],
       [ 0.42374811],
       [ 0.25303404],
       [ 0.08999016],
       [ 0.02471484],
       [-0.05592307]]), -0.11729512576545988]
At round 8 accuracy: 0.42952380952380953
At round 8 training accuracy: 0.4440816326530612
At round 8 training loss: 0.41054563437189373
gradient difference: 0.0009829001980940738
At round 8 model param: 
[array([[-2.02197429],
       [ 1.01014739],
       [ 0.8304027 ],
       [ 0.42007094],
       [ 0.25238772],
       [ 0.09094279],
       [ 0.02521346],
       [-0.05830582]]), -0.11916834435292653]
At round 9 accuracy: 0.42952380952380953
At round 9 training accuracy: 0.4440816326530612
At round 9 training loss: 0.41054508090019226
gradient difference: 0.0009804899700292919
At round 9 model param: 
[array([[-2.01754877],
       [ 1.01666577],
       [ 0.83422009],
       [ 0.42176677],
       [ 0.25502585],
       [ 0.09454474],
       [ 0.02868841],
       [-0.05502007]]), -0.11690131894179753]
Total training time: 54.94809079170227s
At round 10 accuracy: 0.42952380952380953
At round 10 training accuracy: 0.4440816326530612
At round 10 model param: 
[array([[-2.01754877],
       [ 1.01666577],
       [ 0.83422009],
       [ 0.42176677],
       [ 0.25502585],
       [ 0.09454474],
       [ 0.02868841],
       [-0.05502007]]), -0.11690131894179753]
