Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 38
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.43238095238095237
At round 0 training accuracy: 0.4304081632653061
At round 0 training loss: 0.6931464076042175
gradient difference: 0.0011569165271970783
At round 0 model param: 
[array([[-1.72159728],
       [ 0.84985491],
       [ 0.7581    ],
       [ 0.35070129],
       [ 0.15609161],
       [ 0.0197762 ],
       [-0.01984147],
       [ 0.05252852]]), 0.026897776206689223]
At round 1 accuracy: 0.43238095238095237
At round 1 training accuracy: 0.4304081632653061
At round 1 training loss: 0.4207520910671779
gradient difference: 0.0007584924435470029
At round 1 model param: 
[array([[-1.89641743],
       [ 0.93334184],
       [ 0.8221296 ],
       [ 0.38542207],
       [ 0.16562768],
       [ 0.01184143],
       [-0.02419076],
       [ 0.04558878]]), 0.010637458946023668]
At round 2 accuracy: 0.43238095238095237
At round 2 training accuracy: 0.4304081632653061
At round 2 training loss: 0.4190261449132647
gradient difference: 0.0007687796274446307
At round 2 model param: 
[array([[-1.9315224 ],
       [ 0.9480648 ],
       [ 0.83349572],
       [ 0.39189181],
       [ 0.17183132],
       [ 0.016827  ],
       [-0.02624067],
       [ 0.04715024]]), 0.0025066124009234564]
At round 3 accuracy: 0.43238095238095237
At round 3 training accuracy: 0.4304081632653061
At round 3 training loss: 0.41896091188703266
gradient difference: 0.0007708039147078256
At round 3 model param: 
[array([[-1.93099584],
       [ 0.95556828],
       [ 0.84279513],
       [ 0.39631575],
       [ 0.17823848],
       [ 0.01818228],
       [-0.02049461],
       [ 0.04913758]]), 0.0006554744073322841]
At round 4 accuracy: 0.43238095238095237
At round 4 training accuracy: 0.4304081632653061
At round 4 training loss: 0.41904181241989136
gradient difference: 0.0007858846085031379
At round 4 model param: 
[array([[-1.93720266],
       [ 0.95190666],
       [ 0.83888607],
       [ 0.39885348],
       [ 0.17633063],
       [ 0.01476183],
       [-0.02347734],
       [ 0.04852251]]), -0.004793952485280377]
At round 5 accuracy: 0.43238095238095237
At round 5 training accuracy: 0.4304081632653061
At round 5 training loss: 0.4189613546643938
gradient difference: 0.000776511579563772
At round 5 model param: 
[array([[-1.93954287],
       [ 0.95194851],
       [ 0.83717362],
       [ 0.39541419],
       [ 0.17485617],
       [ 0.01279862],
       [-0.02384452],
       [ 0.04604125]]), -0.009012368374637194]
At round 6 accuracy: 0.43238095238095237
At round 6 training accuracy: 0.4304081632653061
At round 6 training loss: 0.4189625084400177
gradient difference: 0.0007713644370221834
At round 6 model param: 
[array([[-1.931844  ],
       [ 0.95746839],
       [ 0.84626572],
       [ 0.3997774 ],
       [ 0.17858439],
       [ 0.02080627],
       [-0.01737753],
       [ 0.05381657]]), -0.0037700702835406575]
At round 7 accuracy: 0.43238095238095237
At round 7 training accuracy: 0.4304081632653061
At round 7 training loss: 0.41913221563611713
gradient difference: 0.0007901869093169415
At round 7 model param: 
[array([[-1.93990842],
       [ 0.9516004 ],
       [ 0.83690507],
       [ 0.39393667],
       [ 0.17378637],
       [ 0.01363536],
       [-0.02418168],
       [ 0.04602219]]), -0.011043695001197713]
At round 8 accuracy: 0.43238095238095237
At round 8 training accuracy: 0.4304081632653061
At round 8 training loss: 0.41897150448390413
gradient difference: 0.0007689120151751083
At round 8 model param: 
[array([[-1.93456462],
       [ 0.95534096],
       [ 0.84208846],
       [ 0.399063  ],
       [ 0.17719407],
       [ 0.01823422],
       [-0.02254326],
       [ 0.04955795]]), -0.007635048324508327]
At round 9 accuracy: 0.43238095238095237
At round 9 training accuracy: 0.4304081632653061
At round 9 training loss: 0.4189911016396114
gradient difference: 0.0007825832179759335
At round 9 model param: 
[array([[-1.94044973],
       [ 0.95142649],
       [ 0.83913187],
       [ 0.39370232],
       [ 0.17304544],
       [ 0.0148521 ],
       [-0.02519984],
       [ 0.04282432]]), -0.011862074158021383]
Total training time: 39.995681047439575s
At round 10 accuracy: 0.43238095238095237
At round 10 training accuracy: 0.4304081632653061
At round 10 model param: 
[array([[-1.94044973],
       [ 0.95142649],
       [ 0.83913187],
       [ 0.39370232],
       [ 0.17304544],
       [ 0.0148521 ],
       [-0.02519984],
       [ 0.04282432]]), -0.011862074158021383]
