Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 42
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.43142857142857144
At round 0 training accuracy: 0.4328571428571429
At round 0 training loss: 0.6931464076042175
gradient difference: 0.0033617113549194166
At round 0 model param: 
[array([[-1.78738693],
       [ 0.86578024],
       [ 0.74602158],
       [ 0.41151653],
       [ 0.20229932],
       [ 0.04013511],
       [-0.01223022],
       [ 0.00295214]]), 0.020933406693594798]
At round 1 accuracy: 0.43142857142857144
At round 1 training accuracy: 0.4328571428571429
At round 1 training loss: 0.40807574135916574
gradient difference: 0.0007485728266700987
At round 1 model param: 
[array([[-1.98389452],
       [ 0.94844302],
       [ 0.81817576],
       [ 0.44300566],
       [ 0.20776007],
       [ 0.03910048],
       [-0.017802  ],
       [-0.01534866]]), -0.006218524716262307]
At round 2 accuracy: 0.43142857142857144
At round 2 training accuracy: 0.4328571428571429
At round 2 training loss: 0.4063998886517116
gradient difference: 0.0006798586400875084
At round 2 model param: 
[array([[-2.02300615],
       [ 0.97069458],
       [ 0.83164544],
       [ 0.45650285],
       [ 0.2122448 ],
       [ 0.03980451],
       [-0.01857081],
       [-0.01674524]]), -0.02000946711216654]
At round 3 accuracy: 0.43142857142857144
At round 3 training accuracy: 0.4328571428571429
At round 3 training loss: 0.40639167598315645
gradient difference: 0.0006770867245381004
At round 3 model param: 
[array([[-2.02720092],
       [ 0.97974784],
       [ 0.83952798],
       [ 0.45935346],
       [ 0.21795293],
       [ 0.04596   ],
       [-0.01211255],
       [-0.01041042]]), -0.02497317242835249]
At round 4 accuracy: 0.43142857142857144
At round 4 training accuracy: 0.4328571428571429
At round 4 training loss: 0.40628056866782053
gradient difference: 0.0006924792480896232
At round 4 model param: 
[array([[-2.0233329 ],
       [ 0.98375198],
       [ 0.84680565],
       [ 0.46553273],
       [ 0.22187827],
       [ 0.05018323],
       [-0.01207941],
       [-0.01109162]]), -0.02699186440025057]
At round 5 accuracy: 0.43142857142857144
At round 5 training accuracy: 0.4328571428571429
At round 5 training loss: 0.4063099367277963
gradient difference: 0.0007102901429420252
At round 5 model param: 
[array([[-2.02731868],
       [ 0.97882833],
       [ 0.84448785],
       [ 0.46220217],
       [ 0.22003347],
       [ 0.04486061],
       [-0.01006993],
       [-0.01085122]]), -0.03247965433235679]
At round 6 accuracy: 0.43142857142857144
At round 6 training accuracy: 0.4328571428571429
At round 6 training loss: 0.4062811349119459
gradient difference: 0.0006949163082960426
At round 6 model param: 
[array([[-2.02743833],
       [ 0.98117234],
       [ 0.84502845],
       [ 0.46247618],
       [ 0.22008193],
       [ 0.04439635],
       [-0.00953906],
       [-0.0077174 ]]), -0.03346566430159977]
At round 7 accuracy: 0.43142857142857144
At round 7 training accuracy: 0.4328571428571429
At round 7 training loss: 0.4062825100762503
gradient difference: 0.0006969309637602964
At round 7 model param: 
[array([[-2.03450504],
       [ 0.9775894 ],
       [ 0.83654306],
       [ 0.45557615],
       [ 0.21393888],
       [ 0.03939296],
       [-0.01751523],
       [-0.01887367]]), -0.041013739071786404]
At round 8 accuracy: 0.43142857142857144
At round 8 training accuracy: 0.4328571428571429
At round 8 training loss: 0.4065591096878052
gradient difference: 0.0006684177038979302
At round 8 model param: 
[array([[-2.02961561],
       [ 0.97951024],
       [ 0.84201165],
       [ 0.46071532],
       [ 0.2191282 ],
       [ 0.04443443],
       [-0.01248205],
       [-0.01254845]]), -0.037457865729395835]
At round 9 accuracy: 0.43142857142857144
At round 9 training accuracy: 0.4328571428571429
At round 9 training loss: 0.40630812304360525
gradient difference: 0.0006878343291073719
At round 9 model param: 
[array([[-2.02271894],
       [ 0.9834037 ],
       [ 0.84680367],
       [ 0.46555188],
       [ 0.22489371],
       [ 0.04748103],
       [-0.01073976],
       [-0.00367222]]), -0.03380288529608931]
Total training time: 46.521071910858154s
At round 10 accuracy: 0.43142857142857144
At round 10 training accuracy: 0.4328571428571429
At round 10 model param: 
[array([[-2.02271894],
       [ 0.9834037 ],
       [ 0.84680367],
       [ 0.46555188],
       [ 0.22489371],
       [ 0.04748103],
       [-0.01073976],
       [-0.00367222]]), -0.03380288529608931]
