Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 13
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.43523809523809526
At round 0 training accuracy: 0.43551020408163266
At round 0 training loss: 0.6931464076042175
gradient difference: 0.002541279463792887
At round 0 model param: 
[array([[-1.75629706],
       [ 0.92560338],
       [ 0.70191119],
       [ 0.3811977 ],
       [ 0.14935837],
       [ 0.11339413],
       [-0.07819029],
       [-0.00646768]]), 0.05943440459668636]
At round 1 accuracy: 0.43523809523809526
At round 1 training accuracy: 0.43551020408163266
At round 1 training loss: 0.4155171385833195
gradient difference: 0.0014771330591410283
At round 1 model param: 
[array([[-1.95424931],
       [ 1.01871749],
       [ 0.76544443],
       [ 0.41061295],
       [ 0.16174866],
       [ 0.11900624],
       [-0.09474747],
       [-0.01522131]]), 0.07489398866891861]
At round 2 accuracy: 0.43523809523809526
At round 2 training accuracy: 0.43551020408163266
At round 2 training loss: 0.41369955454553875
gradient difference: 0.0016096940485222262
At round 2 model param: 
[array([[-1.99152081],
       [ 1.04106149],
       [ 0.78442941],
       [ 0.41914977],
       [ 0.16632164],
       [ 0.12183336],
       [-0.08896311],
       [-0.0162404 ]]), 0.09041270294359752]
At round 3 accuracy: 0.43523809523809526
At round 3 training accuracy: 0.43551020408163266
At round 3 training loss: 0.4136479028633663
gradient difference: 0.0016220800116905083
At round 3 model param: 
[array([[-2.00630477],
       [ 1.04070328],
       [ 0.78245246],
       [ 0.4163346 ],
       [ 0.16013483],
       [ 0.12152358],
       [-0.095391  ],
       [-0.02030946]]), 0.09314611767019544]
At round 4 accuracy: 0.43523809523809526
At round 4 training accuracy: 0.43551020408163266
At round 4 training loss: 0.4136409248624529
gradient difference: 0.0016490796430687644
At round 4 model param: 
[array([[-2.00281818],
       [ 1.04515576],
       [ 0.78911581],
       [ 0.42059513],
       [ 0.1633599 ],
       [ 0.12420191],
       [-0.09212832],
       [-0.01303726]]), 0.10166827695710319]
At round 5 accuracy: 0.43523809523809526
At round 5 training accuracy: 0.43551020408163266
At round 5 training loss: 0.41367241740226746
gradient difference: 0.0016260508545627441
At round 5 model param: 
[array([[-2.00460958],
       [ 1.04844913],
       [ 0.78472966],
       [ 0.42074552],
       [ 0.16300306],
       [ 0.12167394],
       [-0.0952625 ],
       [-0.01706392]]), 0.1024216285773686]
At round 6 accuracy: 0.43523809523809526
At round 6 training accuracy: 0.43551020408163266
At round 6 training loss: 0.41363589252744404
gradient difference: 0.0016296270425849955
At round 6 model param: 
[array([[-2.01335205],
       [ 1.03723194],
       [ 0.78194762],
       [ 0.41228618],
       [ 0.15690615],
       [ 0.11020298],
       [-0.09792881],
       [-0.02059209]]), 0.09772099767412458]
At round 7 accuracy: 0.43523809523809526
At round 7 training accuracy: 0.43551020408163266
At round 7 training loss: 0.4137802507196154
gradient difference: 0.0016612271176328717
At round 7 model param: 
[array([[-2.00623168],
       [ 1.04631994],
       [ 0.78565993],
       [ 0.42009936],
       [ 0.16191347],
       [ 0.12075199],
       [-0.09607862],
       [-0.01573078]]), 0.10443482654435295]
At round 8 accuracy: 0.43523809523809526
At round 8 training accuracy: 0.43551020408163266
At round 8 training loss: 0.41363111989838736
gradient difference: 0.001632021396949748
At round 8 model param: 
[array([[-2.01457058],
       [ 1.03608085],
       [ 0.77858004],
       [ 0.41507562],
       [ 0.15768761],
       [ 0.11465322],
       [-0.10172236],
       [-0.02156303]]), 0.09860686319214958]
At round 9 accuracy: 0.43523809523809526
At round 9 training accuracy: 0.43551020408163266
At round 9 training loss: 0.41379216739109587
gradient difference: 0.001664227805634027
At round 9 model param: 
[array([[-2.00329709],
       [ 1.04650537],
       [ 0.78795596],
       [ 0.42078108],
       [ 0.16778772],
       [ 0.12655039],
       [-0.09404744],
       [-0.01476602]]), 0.10706128286463874]
Total training time: 40.822468996047974s
At round 10 accuracy: 0.43523809523809526
At round 10 training accuracy: 0.43551020408163266
At round 10 model param: 
[array([[-2.00329709],
       [ 1.04650537],
       [ 0.78795596],
       [ 0.42078108],
       [ 0.16778772],
       [ 0.12655039],
       [-0.09404744],
       [-0.01476602]]), 0.10706128286463874]
