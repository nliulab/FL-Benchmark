Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 1
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.42428571428571427
At round 0 training accuracy: 0.4406122448979592
At round 0 training loss: 0.6931464076042175
gradient difference: 0.001802492097595002
At round 0 model param: 
[array([[-1.77791549],
       [ 0.84548162],
       [ 0.72057157],
       [ 0.35920918],
       [ 0.20184348],
       [ 0.05313368],
       [-0.0410146 ],
       [ 0.06017966]]), 0.04084346017667225]
At round 1 accuracy: 0.42428571428571427
At round 1 training accuracy: 0.4406122448979592
At round 1 training loss: 0.4117593211787088
gradient difference: 0.0006950677204339801
At round 1 model param: 
[array([[-1.96083462],
       [ 0.92836584],
       [ 0.7963839 ],
       [ 0.39469295],
       [ 0.21828571],
       [ 0.05272494],
       [-0.05501292],
       [ 0.05984139]]), 0.044461925114904134]
At round 2 accuracy: 0.42428571428571427
At round 2 training accuracy: 0.4406122448979592
At round 2 training loss: 0.409854348216738
gradient difference: 0.0007073401827068
At round 2 model param: 
[array([[-1.99521123],
       [ 0.94696659],
       [ 0.81293714],
       [ 0.40049663],
       [ 0.22241999],
       [ 0.05466975],
       [-0.05589909],
       [ 0.06515486]]), 0.048486960253545215]
At round 3 accuracy: 0.42428571428571427
At round 3 training accuracy: 0.4406122448979592
At round 3 training loss: 0.40979062233652386
gradient difference: 0.0007111855822583553
At round 3 model param: 
[array([[-2.01421138],
       [ 0.93936709],
       [ 0.80596324],
       [ 0.39578635],
       [ 0.21329172],
       [ 0.04790922],
       [-0.06627758],
       [ 0.05664856]]), 0.04143779831273215]
At round 4 accuracy: 0.42428571428571427
At round 4 training accuracy: 0.4406122448979592
At round 4 training loss: 0.4099752902984619
gradient difference: 0.0007093585113412552
At round 4 model param: 
[array([[-2.00937928],
       [ 0.9469556 ],
       [ 0.81021413],
       [ 0.40361936],
       [ 0.22039033],
       [ 0.05140462],
       [-0.05988444],
       [ 0.06126478]]), 0.04759244727236884]
At round 5 accuracy: 0.42428571428571427
At round 5 training accuracy: 0.4406122448979592
At round 5 training loss: 0.4097384086677006
gradient difference: 0.0007112179113887033
At round 5 model param: 
[array([[-2.0069016 ],
       [ 0.94940513],
       [ 0.81103471],
       [ 0.40253259],
       [ 0.22360603],
       [ 0.05661021],
       [-0.05720202],
       [ 0.06423323]]), 0.05050898236887796]
At round 6 accuracy: 0.42428571428571427
At round 6 training accuracy: 0.4406122448979592
At round 6 training loss: 0.40975589411599295
gradient difference: 0.0007118305618287217
At round 6 model param: 
[array([[-2.01016457],
       [ 0.94621631],
       [ 0.80973407],
       [ 0.3997745 ],
       [ 0.2214844 ],
       [ 0.05003124],
       [-0.06210979],
       [ 0.05955352]]), 0.04729247199637549]
At round 7 accuracy: 0.42428571428571427
At round 7 training accuracy: 0.4406122448979592
At round 7 training loss: 0.4097543316228049
gradient difference: 0.0007119456440612139
At round 7 model param: 
[array([[-2.00869152],
       [ 0.94820502],
       [ 0.81095735],
       [ 0.40281037],
       [ 0.2210951 ],
       [ 0.05175322],
       [-0.05943068],
       [ 0.06155801]]), 0.048976246799741475]
At round 8 accuracy: 0.42428571428571427
At round 8 training accuracy: 0.4406122448979592
At round 8 training loss: 0.4097353390284947
gradient difference: 0.0007115888555798126
At round 8 model param: 
[array([[-2.01003088],
       [ 0.94553146],
       [ 0.80880959],
       [ 0.4029267 ],
       [ 0.21874627],
       [ 0.05626452],
       [-0.06032157],
       [ 0.06276921]]), 0.04855955604996]
At round 9 accuracy: 0.42428571428571427
At round 9 training accuracy: 0.4406122448979592
At round 9 training loss: 0.40973775727408274
gradient difference: 0.0007100093254798989
At round 9 model param: 
[array([[-2.00786853],
       [ 0.94885777],
       [ 0.81115888],
       [ 0.4028223 ],
       [ 0.22278455],
       [ 0.04980566],
       [-0.06042666],
       [ 0.06300809]]), 0.049272392477308004]
Total training time: 42.78893280029297s
At round 10 accuracy: 0.42428571428571427
At round 10 training accuracy: 0.4406122448979592
At round 10 model param: 
[array([[-2.00786853],
       [ 0.94885777],
       [ 0.81115888],
       [ 0.4028223 ],
       [ 0.22278455],
       [ 0.04980566],
       [-0.06042666],
       [ 0.06300809]]), 0.049272392477308004]
