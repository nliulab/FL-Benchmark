Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 15
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.4380952380952381
At round 0 training accuracy: 0.4275510204081633
At round 0 training loss: 0.6931464076042175
gradient difference: 0.005186324858515454
At round 0 model param: 
[array([[-1.7820963 ],
       [ 0.92262331],
       [ 0.7182186 ],
       [ 0.29355348],
       [ 0.14039729],
       [ 0.09349037],
       [ 0.01131257],
       [ 0.00470303]]), 0.11214416208011764]
At round 1 accuracy: 0.4380952380952381
At round 1 training accuracy: 0.4275510204081633
At round 1 training loss: 0.4077611906187875
gradient difference: 0.003479118965978256
At round 1 model param: 
[array([[-1.97891756],
       [ 1.00987776],
       [ 0.78523387],
       [ 0.32662846],
       [ 0.15182015],
       [ 0.09848565],
       [ 0.0080847 ],
       [ 0.00724027]]), 0.15584239895854676]
At round 2 accuracy: 0.4380952380952381
At round 2 training accuracy: 0.4275510204081633
At round 2 training loss: 0.4059950241020748
gradient difference: 0.003438046022323124
At round 2 model param: 
[array([[-2.02128901],
       [ 1.02617821],
       [ 0.79903127],
       [ 0.32692334],
       [ 0.15287295],
       [ 0.09472781],
       [ 0.00563024],
       [ 0.00446934]]), 0.17930192606789724]
At round 3 accuracy: 0.4380952380952381
At round 3 training accuracy: 0.4275510204081633
At round 3 training loss: 0.40590098925999235
gradient difference: 0.003432092463469531
At round 3 model param: 
[array([[-2.03288024e+00],
       [ 1.02858019e+00],
       [ 8.01342692e-01],
       [ 3.26396589e-01],
       [ 1.49711030e-01],
       [ 9.27794940e-02],
       [ 2.05621128e-03],
       [-2.86128904e-04]]), 0.19233384089810507]
At round 4 accuracy: 0.4380952380952381
At round 4 training accuracy: 0.4275510204081633
At round 4 training loss: 0.4058785225663866
gradient difference: 0.003434175809034296
At round 4 model param: 
[array([[-2.03122234],
       [ 1.03177479],
       [ 0.80394366],
       [ 0.32783521],
       [ 0.15519163],
       [ 0.09938584],
       [ 0.00432882],
       [ 0.00543429]]), 0.20509999777589524]
At round 5 accuracy: 0.4380952380952381
At round 5 training accuracy: 0.4275510204081633
At round 5 training loss: 0.4060246901852744
gradient difference: 0.0034293502493850142
At round 5 model param: 
[array([[-2.04226892],
       [ 1.02327135],
       [ 0.79496732],
       [ 0.32653983],
       [ 0.14488104],
       [ 0.08583248],
       [-0.00629978],
       [-0.00483814]]), 0.202164249760764]
At round 6 accuracy: 0.4380952380952381
At round 6 training accuracy: 0.4275510204081633
At round 6 training loss: 0.40595813734190805
gradient difference: 0.003446309301684462
At round 6 model param: 
[array([[-2.03931172e+00],
       [ 1.02665738e+00],
       [ 7.97476709e-01],
       [ 3.25413299e-01],
       [ 1.47509754e-01],
       [ 9.17647353e-02],
       [-4.21629207e-03],
       [-6.65710973e-04]]), 0.20775937182562693]
At round 7 accuracy: 0.4380952380952381
At round 7 training accuracy: 0.4275510204081633
At round 7 training loss: 0.4058782288006374
gradient difference: 0.0034399809616513645
At round 7 model param: 
[array([[-2.03696465e+00],
       [ 1.02688279e+00],
       [ 7.99859439e-01],
       [ 3.23478001e-01],
       [ 1.48962087e-01],
       [ 9.09992104e-02],
       [-1.72220596e-03],
       [-3.58929246e-03]]), 0.21007383721215384]
At round 8 accuracy: 0.4380952380952381
At round 8 training accuracy: 0.4275510204081633
At round 8 training loss: 0.40587659393038067
gradient difference: 0.003436828712575314
At round 8 model param: 
[array([[-2.03571054e+00],
       [ 1.02773778e+00],
       [ 7.98918690e-01],
       [ 3.26770531e-01],
       [ 1.49228905e-01],
       [ 9.69019598e-02],
       [ 3.81623714e-03],
       [-1.27105867e-03]]), 0.21310073563030787]
At round 9 accuracy: 0.4380952380952381
At round 9 training accuracy: 0.4275510204081633
At round 9 training loss: 0.4059124503816877
gradient difference: 0.003428479984142342
At round 9 model param: 
[array([[-2.03477543],
       [ 1.02967237],
       [ 0.80069847],
       [ 0.32858478],
       [ 0.14985403],
       [ 0.09628406],
       [ 0.00209392],
       [ 0.00373438]]), 0.2145820004599435]
Total training time: 40.196568727493286s
At round 10 accuracy: 0.4380952380952381
At round 10 training accuracy: 0.4275510204081633
At round 10 model param: 
[array([[-2.03477543],
       [ 1.02967237],
       [ 0.80069847],
       [ 0.32858478],
       [ 0.14985403],
       [ 0.09628406],
       [ 0.00209392],
       [ 0.00373438]]), 0.2145820004599435]
