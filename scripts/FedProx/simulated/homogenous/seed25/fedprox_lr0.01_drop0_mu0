Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 25
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.44142857142857145
At round 0 training accuracy: 0.45
At round 0 training loss: 0.6931464076042175
gradient difference: 0.0020937344027894705
At round 0 model param: 
[array([[-1.85152279],
       [ 0.9087513 ],
       [ 0.78371895],
       [ 0.38298289],
       [ 0.19157037],
       [ 0.079835  ],
       [-0.0060557 ],
       [-0.0234511 ]]), 0.0394832438656262]
At round 1 accuracy: 0.44142857142857145
At round 1 training accuracy: 0.45
At round 1 training loss: 0.39209477390561787
gradient difference: 0.0014009060780659143
At round 1 model param: 
[array([[-2.06762876],
       [ 1.00649873],
       [ 0.86148159],
       [ 0.41756461],
       [ 0.20917014],
       [ 0.07394968],
       [-0.02215412],
       [-0.03960763]]), 0.03057824288095747]
At round 2 accuracy: 0.44142857142857145
At round 2 training accuracy: 0.45
At round 2 training loss: 0.3896369082587106
gradient difference: 0.0013672503768554294
At round 2 model param: 
[array([[-2.11094747],
       [ 1.03438415],
       [ 0.88250855],
       [ 0.4318816 ],
       [ 0.21805743],
       [ 0.07795968],
       [-0.02429315],
       [-0.03840836]]), 0.03005118348768779]
At round 3 accuracy: 0.44142857142857145
At round 3 training accuracy: 0.45
At round 3 training loss: 0.3894617898123605
gradient difference: 0.0013738005839487118
At round 3 model param: 
[array([[-2.12113919],
       [ 1.04330931],
       [ 0.88550499],
       [ 0.43391735],
       [ 0.21885417],
       [ 0.08131684],
       [-0.02121432],
       [-0.0386395 ]]), 0.02842838955777032]
At round 4 accuracy: 0.44142857142857145
At round 4 training accuracy: 0.45
At round 4 training loss: 0.38944901313100544
gradient difference: 0.0013779638110517807
At round 4 model param: 
[array([[-2.12429714],
       [ 1.04052317],
       [ 0.88959936],
       [ 0.43724156],
       [ 0.22063945],
       [ 0.07939836],
       [-0.02423404],
       [-0.03724523]]), 0.026968596237046377]
At round 5 accuracy: 0.44142857142857145
At round 5 training accuracy: 0.45
At round 5 training loss: 0.389452908720289
gradient difference: 0.0013750701348178044
At round 5 model param: 
[array([[-2.12595075],
       [ 1.0438682 ],
       [ 0.88800386],
       [ 0.4348788 ],
       [ 0.22212109],
       [ 0.08055877],
       [-0.02377243],
       [-0.03698132]]), 0.02666655874678067]
At round 6 accuracy: 0.44142857142857145
At round 6 training accuracy: 0.45
At round 6 training loss: 0.38944933244160246
gradient difference: 0.00137585235930548
At round 6 model param: 
[array([[-2.12500136],
       [ 1.04313949],
       [ 0.88981316],
       [ 0.43410189],
       [ 0.2177993 ],
       [ 0.08037346],
       [-0.02267085],
       [-0.04008267]]), 0.02538560117994036]
At round 7 accuracy: 0.44142857142857145
At round 7 training accuracy: 0.45
At round 7 training loss: 0.38944797004972187
gradient difference: 0.0013754536842832399
At round 7 model param: 
[array([[-2.13058598],
       [ 1.03967411],
       [ 0.88523797],
       [ 0.42903325],
       [ 0.21404358],
       [ 0.0772965 ],
       [-0.02735156],
       [-0.04401252]]), 0.021486685744353702]
At round 8 accuracy: 0.44142857142857145
At round 8 training accuracy: 0.45
At round 8 training loss: 0.3895584983485086
gradient difference: 0.0013661316130207813
At round 8 model param: 
[array([[-2.12663354],
       [ 1.04367219],
       [ 0.88686979],
       [ 0.43393056],
       [ 0.21793748],
       [ 0.0812534 ],
       [-0.02253264],
       [-0.04003854]]), 0.02443829338465418]
At round 9 accuracy: 0.44142857142857145
At round 9 training accuracy: 0.45
At round 9 training loss: 0.38944895778383526
gradient difference: 0.0013750157322958338
At round 9 model param: 
[array([[-2.12656549],
       [ 1.04058049],
       [ 0.88955864],
       [ 0.43186545],
       [ 0.21671174],
       [ 0.07890728],
       [-0.02393899],
       [-0.03814253]]), 0.023111232157264436]
Total training time: 46.42678999900818s
At round 10 accuracy: 0.44142857142857145
At round 10 training accuracy: 0.45
At round 10 model param: 
[array([[-2.12656549],
       [ 1.04058049],
       [ 0.88955864],
       [ 0.43186545],
       [ 0.21671174],
       [ 0.07890728],
       [-0.02393899],
       [-0.03814253]]), 0.023111232157264436]
