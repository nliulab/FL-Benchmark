Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 23
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.42142857142857143
At round 0 training accuracy: 0.43081632653061225
At round 0 training loss: 0.6931464076042175
gradient difference: 0.0023677157553711283
At round 0 model param: 
[array([[-1.714939  ],
       [ 0.87846827],
       [ 0.68253048],
       [ 0.28886591],
       [ 0.20172673],
       [ 0.17073606],
       [ 0.03155802],
       [ 0.01483651]]), -0.0454740726522037]
At round 1 accuracy: 0.42142857142857143
At round 1 training accuracy: 0.43081632653061225
At round 1 training loss: 0.4210522600582668
gradient difference: 0.0019608505238116448
At round 1 model param: 
[array([[-1.88675213],
       [ 0.96255583],
       [ 0.75282189],
       [ 0.32439455],
       [ 0.22949409],
       [ 0.19381019],
       [ 0.03156881],
       [ 0.01953577]]), -0.08723932398217064]
At round 2 accuracy: 0.42142857142857143
At round 2 training accuracy: 0.43081632653061225
At round 2 training loss: 0.41943104352269855
gradient difference: 0.0019427494522467664
At round 2 model param: 
[array([[-1.92373284],
       [ 0.97758902],
       [ 0.75903088],
       [ 0.32999106],
       [ 0.23132108],
       [ 0.19055315],
       [ 0.02908513],
       [ 0.01461258]]), -0.11733439471572638]
At round 3 accuracy: 0.42142857142857143
At round 3 training accuracy: 0.43081632653061225
At round 3 training loss: 0.41942944271223886
gradient difference: 0.0020109327013902254
At round 3 model param: 
[array([[-1.92228654],
       [ 0.9886229 ],
       [ 0.76786567],
       [ 0.33542415],
       [ 0.23462247],
       [ 0.19599465],
       [ 0.03696053],
       [ 0.01808729]]), -0.12882358421172416]
At round 4 accuracy: 0.42142857142857143
At round 4 training accuracy: 0.43081632653061225
At round 4 training loss: 0.4193551880972726
gradient difference: 0.001959137443080249
At round 4 model param: 
[array([[-1.92264862],
       [ 0.9883501 ],
       [ 0.77345428],
       [ 0.33851016],
       [ 0.23758161],
       [ 0.19764592],
       [ 0.03857948],
       [ 0.02357126]]), -0.13558782690337726]
At round 5 accuracy: 0.42142857142857143
At round 5 training accuracy: 0.43081632653061225
At round 5 training loss: 0.4193756708077022
gradient difference: 0.0019337038994717288
At round 5 model param: 
[array([[-1.92274691],
       [ 0.98992802],
       [ 0.77194852],
       [ 0.33573057],
       [ 0.23924096],
       [ 0.20187599],
       [ 0.03869205],
       [ 0.02341718]]), -0.14091776417834417]
At round 6 accuracy: 0.42142857142857143
At round 6 training accuracy: 0.43081632653061225
At round 6 training loss: 0.41937468307358877
gradient difference: 0.0019364530442364438
At round 6 model param: 
[array([[-1.92384594],
       [ 0.9870872 ],
       [ 0.7729795 ],
       [ 0.33716513],
       [ 0.23714754],
       [ 0.20096164],
       [ 0.03760448],
       [ 0.02571501]]), -0.14451620514903749]
At round 7 accuracy: 0.42142857142857143
At round 7 training accuracy: 0.43081632653061225
At round 7 training loss: 0.4193668280329023
gradient difference: 0.0019400621841991674
At round 7 model param: 
[array([[-1.92620781],
       [ 0.98828635],
       [ 0.76863203],
       [ 0.33479136],
       [ 0.23343576],
       [ 0.19693708],
       [ 0.03165262],
       [ 0.01824754]]), -0.14960018226078578]
At round 8 accuracy: 0.42142857142857143
At round 8 training accuracy: 0.43081632653061225
At round 8 training loss: 0.41941720247268677
gradient difference: 0.0019853423284573613
At round 8 model param: 
[array([[-1.92217144],
       [ 0.98917236],
       [ 0.77530555],
       [ 0.33791867],
       [ 0.23766088],
       [ 0.20072362],
       [ 0.03693096],
       [ 0.023729  ]]), -0.1473223524434226]
At round 9 accuracy: 0.42142857142857143
At round 9 training accuracy: 0.43081632653061225
At round 9 training loss: 0.4193707363946097
gradient difference: 0.0019344170225193407
At round 9 model param: 
[array([[-1.92363325],
       [ 0.98742449],
       [ 0.77356523],
       [ 0.33502548],
       [ 0.24021547],
       [ 0.19832415],
       [ 0.03704232],
       [ 0.02077112]]), -0.1487473142998559]
Total training time: 58.77397084236145s
At round 10 accuracy: 0.42142857142857143
At round 10 training accuracy: 0.43081632653061225
At round 10 model param: 
[array([[-1.92363325],
       [ 0.98742449],
       [ 0.77356523],
       [ 0.33502548],
       [ 0.24021547],
       [ 0.19832415],
       [ 0.03704232],
       [ 0.02077112]]), -0.1487473142998559]
