Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 36
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.439047619047619
At round 0 training accuracy: 0.4406122448979592
At round 0 training loss: 0.6931464076042175
gradient difference: 0.002768886746746425
At round 0 model param: 
[array([[-1.82399447],
       [ 0.91723466],
       [ 0.73115301],
       [ 0.29744803],
       [ 0.17547396],
       [ 0.07920187],
       [-0.01958185],
       [ 0.0992794 ]]), 0.015082101237827114]
At round 1 accuracy: 0.439047619047619
At round 1 training accuracy: 0.4406122448979592
At round 1 training loss: 0.4027863996369498
gradient difference: 0.001307960342647089
At round 1 model param: 
[array([[-2.02093094],
       [ 1.0131317 ],
       [ 0.8108764 ],
       [ 0.33205169],
       [ 0.1934453 ],
       [ 0.09039367],
       [-0.01959977],
       [ 0.11118545]]), 0.0016398002127451555]
At round 2 accuracy: 0.439047619047619
At round 2 training accuracy: 0.4406122448979592
At round 2 training loss: 0.4009016326495579
gradient difference: 0.001286875340582746
At round 2 model param: 
[array([[-2.06933693],
       [ 1.03033449],
       [ 0.82455663],
       [ 0.33305684],
       [ 0.1914629 ],
       [ 0.08773411],
       [-0.02480405],
       [ 0.10723457]]), -0.01253343719456877]
At round 3 accuracy: 0.439047619047619
At round 3 training accuracy: 0.4406122448979592
At round 3 training loss: 0.40086932267461506
gradient difference: 0.0013083383288464846
At round 3 model param: 
[array([[-2.07615604],
       [ 1.03833916],
       [ 0.83054181],
       [ 0.33816739],
       [ 0.19621211],
       [ 0.09218651],
       [-0.02272481],
       [ 0.11164639]]), -0.015874710732272694]
At round 4 accuracy: 0.439047619047619
At round 4 training accuracy: 0.4406122448979592
At round 4 training loss: 0.40079293080738615
gradient difference: 0.0012936797136823622
At round 4 model param: 
[array([[-2.07890909],
       [ 1.03928699],
       [ 0.83149813],
       [ 0.33860107],
       [ 0.19738515],
       [ 0.0937163 ],
       [-0.02324283],
       [ 0.11449952]]), -0.01937370507844857]
At round 5 accuracy: 0.439047619047619
At round 5 training accuracy: 0.4406122448979592
At round 5 training loss: 0.4007911724703653
gradient difference: 0.0012907741902544905
At round 5 model param: 
[array([[-2.07750283],
       [ 1.04129779],
       [ 0.83349787],
       [ 0.33904298],
       [ 0.198377  ],
       [ 0.09580654],
       [-0.02143633],
       [ 0.11611267]]), -0.02055639640561172]
At round 6 accuracy: 0.439047619047619
At round 6 training accuracy: 0.4406122448979592
At round 6 training loss: 0.4007873407432011
gradient difference: 0.001284467222935774
At round 6 model param: 
[array([[-2.07955228],
       [ 1.04108352],
       [ 0.8314598 ],
       [ 0.3398193 ],
       [ 0.1954341 ],
       [ 0.09152122],
       [-0.02051858],
       [ 0.11285275]]), -0.023515138775110245]
At round 7 accuracy: 0.439047619047619
At round 7 training accuracy: 0.4406122448979592
At round 7 training loss: 0.40080138615199495
gradient difference: 0.001296332892837095
At round 7 model param: 
[array([[-2.07496449],
       [ 1.04636598],
       [ 0.83319217],
       [ 0.34256446],
       [ 0.19959472],
       [ 0.09720884],
       [-0.01817822],
       [ 0.11696471]]), -0.020642346968608245]
At round 8 accuracy: 0.439047619047619
At round 8 training accuracy: 0.4406122448979592
At round 8 training loss: 0.40081632137298584
gradient difference: 0.0012779833581229507
At round 8 model param: 
[array([[-2.07496885],
       [ 1.04403923],
       [ 0.83395513],
       [ 0.34318123],
       [ 0.2014743 ],
       [ 0.09755085],
       [-0.01670664],
       [ 0.11951501]]), -0.02029708826116153]
At round 9 accuracy: 0.439047619047619
At round 9 training accuracy: 0.4406122448979592
At round 9 training loss: 0.4008349733693259
gradient difference: 0.0012745022436061709
At round 9 model param: 
[array([[-2.07785998],
       [ 1.04108729],
       [ 0.83359795],
       [ 0.34016364],
       [ 0.19883557],
       [ 0.09339746],
       [-0.02334471],
       [ 0.11389695]]), -0.02468498450304781]
Total training time: 41.89919114112854s
At round 10 accuracy: 0.439047619047619
At round 10 training accuracy: 0.4406122448979592
At round 10 model param: 
[array([[-2.07785998],
       [ 1.04108729],
       [ 0.83359795],
       [ 0.34016364],
       [ 0.19883557],
       [ 0.09339746],
       [-0.02334471],
       [ 0.11389695]]), -0.02468498450304781]
