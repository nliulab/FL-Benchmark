Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 28
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.43476190476190474
At round 0 training accuracy: 0.42653061224489797
At round 0 training loss: 0.6931464076042175
gradient difference: 0.0006320968336272775
At round 0 model param: 
[array([[-1.7726958 ],
       [ 0.88885896],
       [ 0.73903584],
       [ 0.34036572],
       [ 0.14860819],
       [ 0.10216638],
       [-0.00716888],
       [-0.02430353]]), 0.07397748849221639]
At round 1 accuracy: 0.43476190476190474
At round 1 training accuracy: 0.42653061224489797
At round 1 training loss: 0.4093665991510664
gradient difference: 0.0009470586556944545
At round 1 model param: 
[array([[-1.94951078],
       [ 0.98503797],
       [ 0.81627052],
       [ 0.38018529],
       [ 0.16929537],
       [ 0.11067754],
       [-0.0053151 ],
       [-0.037679  ]]), 0.09593637447272028]
At round 2 accuracy: 0.43476190476190474
At round 2 training accuracy: 0.42653061224489797
At round 2 training loss: 0.40765864508492605
gradient difference: 0.001001452136440625
At round 2 model param: 
[array([[-1.98533031],
       [ 1.00400325],
       [ 0.83348011],
       [ 0.39046645],
       [ 0.17007823],
       [ 0.11264284],
       [-0.00305955],
       [-0.03908905]]), 0.10663062972681862]
At round 3 accuracy: 0.43476190476190474
At round 3 training accuracy: 0.42653061224489797
At round 3 training loss: 0.40764043586594717
gradient difference: 0.0010099455530289557
At round 3 model param: 
[array([[-1.99341461e+00],
       [ 1.00804618e+00],
       [ 8.34655583e-01],
       [ 3.91443333e-01],
       [ 1.75094366e-01],
       [ 1.11444325e-01],
       [-9.57370802e-04],
       [-3.93648084e-02]]), 0.1129773168691567]
At round 4 accuracy: 0.43476190476190474
At round 4 training accuracy: 0.42653061224489797
At round 4 training loss: 0.4076694633279528
gradient difference: 0.0010152290613769425
At round 4 model param: 
[array([[-1.99792865],
       [ 1.00579854],
       [ 0.83505455],
       [ 0.38824639],
       [ 0.16916398],
       [ 0.10848471],
       [-0.00266481],
       [-0.03926158]]), 0.11396588597978864]
At round 5 accuracy: 0.43476190476190474
At round 5 training accuracy: 0.42653061224489797
At round 5 training loss: 0.4076083259923117
gradient difference: 0.001005365560959181
At round 5 model param: 
[array([[-1.99775273],
       [ 1.00842987],
       [ 0.83516953],
       [ 0.3881927 ],
       [ 0.16944224],
       [ 0.10884085],
       [-0.0040098 ],
       [-0.04264107]]), 0.11585947259196214]
At round 6 accuracy: 0.43476190476190474
At round 6 training accuracy: 0.42653061224489797
At round 6 training loss: 0.4076084622314998
gradient difference: 0.0010086943214964296
At round 6 model param: 
[array([[-2.00220983],
       [ 1.00468479],
       [ 0.82981119],
       [ 0.38494733],
       [ 0.16433691],
       [ 0.10331571],
       [-0.00627691],
       [-0.04834583]]), 0.11258872811283384]
At round 7 accuracy: 0.43476190476190474
At round 7 training accuracy: 0.42653061224489797
At round 7 training loss: 0.40764759268079487
gradient difference: 0.001001324620092694
At round 7 model param: 
[array([[-2.00567831],
       [ 1.00000053],
       [ 0.83017451],
       [ 0.3817045 ],
       [ 0.16283794],
       [ 0.10406273],
       [-0.00942861],
       [-0.04696503]]), 0.1118679977953434]
At round 8 accuracy: 0.43476190476190474
At round 8 training accuracy: 0.42653061224489797
At round 8 training loss: 0.4077174322945731
gradient difference: 0.000995135731650402
At round 8 model param: 
[array([[-1.99879415],
       [ 1.00479746],
       [ 0.83198019],
       [ 0.38791569],
       [ 0.16975169],
       [ 0.11095164],
       [-0.00605982],
       [-0.04327727]]), 0.11671718901821546]
At round 9 accuracy: 0.43476190476190474
At round 9 training accuracy: 0.42653061224489797
At round 9 training loss: 0.4075984443937029
gradient difference: 0.0010096373681936655
At round 9 model param: 
[array([[-2.00099019],
       [ 1.00158211],
       [ 0.8340495 ],
       [ 0.38561271],
       [ 0.16914963],
       [ 0.10717765],
       [-0.00584867],
       [-0.04538301]]), 0.11569486345563616]
Total training time: 40.1120331287384s
At round 10 accuracy: 0.43476190476190474
At round 10 training accuracy: 0.42653061224489797
At round 10 model param: 
[array([[-2.00099019],
       [ 1.00158211],
       [ 0.8340495 ],
       [ 0.38561271],
       [ 0.16914963],
       [ 0.10717765],
       [-0.00584867],
       [-0.04538301]]), 0.11569486345563616]
