Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 29
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.43666666666666665
At round 0 training accuracy: 0.4426530612244898
At round 0 training loss: 0.6931464076042175
gradient difference: 0.0008697887240031969
At round 0 model param: 
[array([[-1.7464624 ],
       [ 0.86455413],
       [ 0.69976633],
       [ 0.36595937],
       [ 0.17970838],
       [ 0.13060457],
       [-0.04085716],
       [-0.00241371]]), -0.02150877458708627]
At round 1 accuracy: 0.43666666666666665
At round 1 training accuracy: 0.4426530612244898
At round 1 training loss: 0.416379200560706
gradient difference: 0.0009399573467786182
At round 1 model param: 
[array([[-1.93434790e+00],
       [ 9.44556713e-01],
       [ 7.73114996e-01],
       [ 3.96704610e-01],
       [ 1.95375685e-01],
       [ 1.45665597e-01],
       [-4.30201830e-02],
       [-1.78047230e-03]]), -0.05446904531813094]
At round 2 accuracy: 0.43666666666666665
At round 2 training accuracy: 0.4426530612244898
At round 2 training loss: 0.414527884551457
gradient difference: 0.000935614378248583
At round 2 model param: 
[array([[-1.96872313e+00],
       [ 9.65021968e-01],
       [ 7.88059320e-01],
       [ 4.05537099e-01],
       [ 2.00262500e-01],
       [ 1.51552558e-01],
       [-4.08516654e-02],
       [ 3.68290448e-04]]), -0.07204021673117365]
At round 3 accuracy: 0.43666666666666665
At round 3 training accuracy: 0.4426530612244898
At round 3 training loss: 0.4144421432699476
gradient difference: 0.0009387076905662366
At round 3 model param: 
[array([[-1.97568148e+00],
       [ 9.70552547e-01],
       [ 7.91424283e-01],
       [ 4.09013437e-01],
       [ 2.05993631e-01],
       [ 1.50293042e-01],
       [-3.91157482e-02],
       [ 1.49179849e-04]]), -0.08398092697773661]
At round 4 accuracy: 0.43666666666666665
At round 4 training accuracy: 0.4426530612244898
At round 4 training loss: 0.414444557258061
gradient difference: 0.0009410019736202834
At round 4 model param: 
[array([[-1.97580501],
       [ 0.97104291],
       [ 0.79560059],
       [ 0.41229124],
       [ 0.20460195],
       [ 0.15499511],
       [-0.03712071],
       [ 0.00521167]]), -0.08882691072566169]
At round 5 accuracy: 0.43666666666666665
At round 5 training accuracy: 0.4426530612244898
At round 5 training loss: 0.41444400378635954
gradient difference: 0.0009428350200217116
At round 5 model param: 
[array([[-1.97717827e+00],
       [ 9.71353454e-01],
       [ 7.96550282e-01],
       [ 4.08554563e-01],
       [ 2.08561342e-01],
       [ 1.53717629e-01],
       [-3.70361379e-02],
       [ 1.71125095e-03]]), -0.09447636508515903]
At round 6 accuracy: 0.43666666666666665
At round 6 training accuracy: 0.4426530612244898
At round 6 training loss: 0.4144482570035117
gradient difference: 0.0009430640296698577
At round 6 model param: 
[array([[-1.98288148e+00],
       [ 9.66655816e-01],
       [ 7.90651100e-01],
       [ 4.07276375e-01],
       [ 2.00563503e-01],
       [ 1.49012704e-01],
       [-4.46662243e-02],
       [-7.50502305e-05]]), -0.10118627494999341]
At round 7 accuracy: 0.43666666666666665
At round 7 training accuracy: 0.4426530612244898
At round 7 training loss: 0.41463699085371836
gradient difference: 0.0009252462522801591
At round 7 model param: 
[array([[-1.9750028 ],
       [ 0.97218665],
       [ 0.79779925],
       [ 0.41366882],
       [ 0.20948591],
       [ 0.15520507],
       [-0.03671658],
       [ 0.0047469 ]]), -0.09674444475344249]
At round 8 accuracy: 0.43666666666666665
At round 8 training accuracy: 0.4426530612244898
At round 8 training loss: 0.4144558991704668
gradient difference: 0.0009449535758841383
At round 8 model param: 
[array([[-1.97620296],
       [ 0.97019362],
       [ 0.7962574 ],
       [ 0.41246676],
       [ 0.20750739],
       [ 0.15424602],
       [-0.03816445],
       [ 0.00455432]]), -0.0989189286317144]
At round 9 accuracy: 0.43666666666666665
At round 9 training accuracy: 0.4426530612244898
At round 9 training loss: 0.4144515267440251
gradient difference: 0.000940118088215777
At round 9 model param: 
[array([[-1.97492470e+00],
       [ 9.72587092e-01],
       [ 7.95415367e-01],
       [ 4.14017456e-01],
       [ 2.05595340e-01],
       [ 1.53173151e-01],
       [-3.48310002e-02],
       [ 1.93051169e-03]]), -0.09912989714315959]
Total training time: 39.07943868637085s
At round 10 accuracy: 0.43666666666666665
At round 10 training accuracy: 0.4426530612244898
At round 10 model param: 
[array([[-1.97492470e+00],
       [ 9.72587092e-01],
       [ 7.95415367e-01],
       [ 4.14017456e-01],
       [ 2.05595340e-01],
       [ 1.53173151e-01],
       [-3.48310002e-02],
       [ 1.93051169e-03]]), -0.09912989714315959]
