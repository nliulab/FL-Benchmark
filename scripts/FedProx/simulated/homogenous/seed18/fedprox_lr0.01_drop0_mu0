Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 18
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.43952380952380954
At round 0 training accuracy: 0.42959183673469387
At round 0 training loss: 0.6931464076042175
gradient difference: 0.0021325021655845045
At round 0 model param: 
[array([[-1.80952621],
       [ 0.92364206],
       [ 0.73960036],
       [ 0.39012733],
       [ 0.18856018],
       [ 0.09148782],
       [-0.059837  ],
       [ 0.02517844]]), 0.06389046779700688]
At round 1 accuracy: 0.43952380952380954
At round 1 training accuracy: 0.42959183673469387
At round 1 training loss: 0.3975361245019095
gradient difference: 0.0005880041236199235
At round 1 model param: 
[array([[-2.00753893],
       [ 1.02102151],
       [ 0.8109306 ],
       [ 0.44243791],
       [ 0.20489707],
       [ 0.10223316],
       [-0.07310396],
       [ 0.02437119]]), 0.08050708472728729]
At round 2 accuracy: 0.43952380952380954
At round 2 training accuracy: 0.42959183673469387
At round 2 training loss: 0.39566617778369356
gradient difference: 0.0005627511578684899
At round 2 model param: 
[array([[-2.0574538 ],
       [ 1.03355527],
       [ 0.82257898],
       [ 0.44726021],
       [ 0.20394178],
       [ 0.09435335],
       [-0.08309819],
       [ 0.01849031]]), 0.08400967930044446]
At round 3 accuracy: 0.43952380952380954
At round 3 training accuracy: 0.42959183673469387
At round 3 training loss: 0.3955330124923161
gradient difference: 0.0005601364646726141
At round 3 model param: 
[array([[-2.06933512],
       [ 1.03769577],
       [ 0.82533419],
       [ 0.44668637],
       [ 0.20396678],
       [ 0.09067492],
       [-0.08462071],
       [ 0.01653461]]), 0.08822250206555639]
At round 4 accuracy: 0.43952380952380954
At round 4 training accuracy: 0.42959183673469387
At round 4 training loss: 0.39557222383362906
gradient difference: 0.0005595914435617972
At round 4 model param: 
[array([[-2.06605063],
       [ 1.04190084],
       [ 0.83066855],
       [ 0.45284778],
       [ 0.20866711],
       [ 0.09929444],
       [-0.07859578],
       [ 0.02213182]]), 0.09617831717644419]
At round 5 accuracy: 0.43952380952380954
At round 5 training accuracy: 0.42959183673469387
At round 5 training loss: 0.39554297498294283
gradient difference: 0.0005570795314672717
At round 5 model param: 
[array([[-2.06763383],
       [ 1.04075468],
       [ 0.8275028 ],
       [ 0.45519304],
       [ 0.20899498],
       [ 0.09719382],
       [-0.08094279],
       [ 0.0172788 ]]), 0.0965343357196876]
At round 6 accuracy: 0.43952380952380954
At round 6 training accuracy: 0.42959183673469387
At round 6 training loss: 0.39552207929747446
gradient difference: 0.0005574448525011516
At round 6 model param: 
[array([[-2.06499491],
       [ 1.04343627],
       [ 0.83171584],
       [ 0.45404384],
       [ 0.21248748],
       [ 0.09778173],
       [-0.07997529],
       [ 0.02444935]]), 0.09975440534097808]
At round 7 accuracy: 0.43952380952380954
At round 7 training accuracy: 0.42959183673469387
At round 7 training loss: 0.3955876869814737
gradient difference: 0.0005567549424531085
At round 7 model param: 
[array([[-2.06077916],
       [ 1.04793797],
       [ 0.83581506],
       [ 0.45761535],
       [ 0.21459892],
       [ 0.10236992],
       [-0.07328745],
       [ 0.0266934 ]]), 0.10376536473631859]
At round 8 accuracy: 0.43952380952380954
At round 8 training accuracy: 0.42959183673469387
At round 8 training loss: 0.3958425905023302
gradient difference: 0.0005552544544438841
At round 8 model param: 
[array([[-2.0723193 ],
       [ 1.03898278],
       [ 0.82638397],
       [ 0.44824991],
       [ 0.20485322],
       [ 0.09210368],
       [-0.08439449],
       [ 0.01803492]]), 0.09490818104573659]
At round 9 accuracy: 0.43952380952380954
At round 9 training accuracy: 0.42959183673469387
At round 9 training loss: 0.3955408036708832
gradient difference: 0.0005589654725218417
At round 9 model param: 
[array([[-2.07018082],
       [ 1.03999216],
       [ 0.82890578],
       [ 0.44968897],
       [ 0.20440374],
       [ 0.09593127],
       [-0.07932438],
       [ 0.02331315]]), 0.09811107787702765]
Total training time: 43.136162996292114s
At round 10 accuracy: 0.43952380952380954
At round 10 training accuracy: 0.42959183673469387
At round 10 model param: 
[array([[-2.07018082],
       [ 1.03999216],
       [ 0.82890578],
       [ 0.44968897],
       [ 0.20440374],
       [ 0.09593127],
       [-0.07932438],
       [ 0.02331315]]), 0.09811107787702765]
