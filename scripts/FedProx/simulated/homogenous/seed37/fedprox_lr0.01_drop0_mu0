Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 37
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.4442857142857143
At round 0 training accuracy: 0.43673469387755104
At round 0 training loss: 0.6931464076042175
gradient difference: 0.001861463205496523
At round 0 model param: 
[array([[-1.74051797],
       [ 0.87687949],
       [ 0.63901683],
       [ 0.30811637],
       [ 0.1366226 ],
       [ 0.1134104 ],
       [-0.0123617 ],
       [ 0.01467481]]), 0.09356702119112015]
At round 1 accuracy: 0.4442857142857143
At round 1 training accuracy: 0.43673469387755104
At round 1 training loss: 0.4207952490874699
gradient difference: 0.0015255204960587382
At round 1 model param: 
[array([[-1.91199299],
       [ 0.96184931],
       [ 0.69826047],
       [ 0.33112648],
       [ 0.13907819],
       [ 0.12211372],
       [-0.01558443],
       [ 0.01082186]]), 0.12452769758445877]
At round 2 accuracy: 0.4442857142857143
At round 2 training accuracy: 0.43673469387755104
At round 2 training loss: 0.41942658594676424
gradient difference: 0.001569967816643928
At round 2 model param: 
[array([[-1.9408703 ],
       [ 0.98109873],
       [ 0.71097658],
       [ 0.33862531],
       [ 0.14542012],
       [ 0.12508021],
       [-0.01419274],
       [ 0.01020373]]), 0.1444417585818363]
At round 3 accuracy: 0.4442857142857143
At round 3 training accuracy: 0.43673469387755104
At round 3 training loss: 0.41951569489070345
gradient difference: 0.001559980935526729
At round 3 model param: 
[array([[-1.95564868],
       [ 0.97776863],
       [ 0.70620844],
       [ 0.32986732],
       [ 0.14215781],
       [ 0.12046463],
       [-0.02306238],
       [ 0.00503087]]), 0.1492664079580988]
At round 4 accuracy: 0.4442857142857143
At round 4 training accuracy: 0.43673469387755104
At round 4 training loss: 0.4194089472293854
gradient difference: 0.0015980237154700672
At round 4 model param: 
[array([[-1.96052652],
       [ 0.97459992],
       [ 0.7051612 ],
       [ 0.32883603],
       [ 0.13350593],
       [ 0.11608453],
       [-0.02581693],
       [ 0.00316794]]), 0.15171731476272857]
At round 5 accuracy: 0.4442857142857143
At round 5 training accuracy: 0.43673469387755104
At round 5 training loss: 0.4195043572357723
gradient difference: 0.0016070961050618972
At round 5 model param: 
[array([[-1.95547576],
       [ 0.97809127],
       [ 0.7087798 ],
       [ 0.3328479 ],
       [ 0.13551095],
       [ 0.12160753],
       [-0.02250528],
       [ 0.00618699]]), 0.1584962923079729]
At round 6 accuracy: 0.4442857142857143
At round 6 training accuracy: 0.43673469387755104
At round 6 training loss: 0.41940902386392864
gradient difference: 0.001585660427609441
At round 6 model param: 
[array([[-1.95416721],
       [ 0.97967156],
       [ 0.7092383 ],
       [ 0.33204946],
       [ 0.1406797 ],
       [ 0.11868951],
       [-0.02026276],
       [ 0.00602852]]), 0.16092095710337162]
At round 7 accuracy: 0.4442857142857143
At round 7 training accuracy: 0.43673469387755104
At round 7 training loss: 0.419416218996048
gradient difference: 0.0015808089836306285
At round 7 model param: 
[array([[-1.96360178e+00],
       [ 9.71751613e-01],
       [ 7.03454716e-01],
       [ 3.28645689e-01],
       [ 1.36267698e-01],
       [ 1.13667956e-01],
       [-2.85562389e-02],
       [-9.61567009e-04]]), 0.15620488034827368]
At round 8 accuracy: 0.4442857142857143
At round 8 training accuracy: 0.43673469387755104
At round 8 training loss: 0.4195735199110849
gradient difference: 0.0016175273459108086
At round 8 model param: 
[array([[-1.95713927],
       [ 0.98152805],
       [ 0.70601353],
       [ 0.3299355 ],
       [ 0.13882148],
       [ 0.12094207],
       [-0.02474762],
       [ 0.00373843]]), 0.16145504798207963]
At round 9 accuracy: 0.4442857142857143
At round 9 training accuracy: 0.43673469387755104
At round 9 training loss: 0.41941369005611967
gradient difference: 0.0015917435367464514
At round 9 model param: 
[array([[-1.95544776],
       [ 0.97771797],
       [ 0.70892827],
       [ 0.33305301],
       [ 0.14096504],
       [ 0.12152805],
       [-0.02060432],
       [ 0.00889321]]), 0.1634304736341749]
Total training time: 38.192161083221436s
At round 10 accuracy: 0.4442857142857143
At round 10 training accuracy: 0.43673469387755104
At round 10 model param: 
[array([[-1.95544776],
       [ 0.97771797],
       [ 0.70892827],
       [ 0.33305301],
       [ 0.14096504],
       [ 0.12152805],
       [-0.02060432],
       [ 0.00889321]]), 0.1634304736341749]
