Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 12
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.4247619047619048
At round 0 training accuracy: 0.43387755102040815
At round 0 training loss: 0.6931464076042175
gradient difference: 0.00134009438466525
At round 0 model param: 
[array([[-1.79777951],
       [ 0.91311458],
       [ 0.66965148],
       [ 0.38665852],
       [ 0.17740666],
       [ 0.06357707],
       [-0.01594308],
       [ 0.00655583]]), 0.049214355647563934]
At round 1 accuracy: 0.4247619047619048
At round 1 training accuracy: 0.43387755102040815
At round 1 training loss: 0.41321987339428495
gradient difference: 0.0014355295755021115
At round 1 model param: 
[array([[-1.9858145 ],
       [ 1.00861954],
       [ 0.74352943],
       [ 0.42228297],
       [ 0.19602031],
       [ 0.07628776],
       [-0.01176469],
       [ 0.01493383]]), 0.045326992869377136]
At round 2 accuracy: 0.4247619047619048
At round 2 training accuracy: 0.43387755102040815
At round 2 training loss: 0.4115535744598934
gradient difference: 0.0014413371378093006
At round 2 model param: 
[array([[-2.02444683],
       [ 1.0281234 ],
       [ 0.7528536 ],
       [ 0.43274426],
       [ 0.1985329 ],
       [ 0.07714625],
       [-0.01394873],
       [ 0.01679143]]), 0.037233419184173853]
At round 3 accuracy: 0.4247619047619048
At round 3 training accuracy: 0.43387755102040815
At round 3 training loss: 0.41150165455681936
gradient difference: 0.0014433228799523563
At round 3 model param: 
[array([[-2.03837473],
       [ 1.02918663],
       [ 0.7503194 ],
       [ 0.42818171],
       [ 0.19634516],
       [ 0.07079814],
       [-0.01654199],
       [ 0.01405285]]), 0.02881759105782424]
At round 4 accuracy: 0.4247619047619048
At round 4 training accuracy: 0.43387755102040815
At round 4 training loss: 0.41165063210896086
gradient difference: 0.0014555043073439754
At round 4 model param: 
[array([[-2.02909453],
       [ 1.04111456],
       [ 0.75932667],
       [ 0.43782911],
       [ 0.20847521],
       [ 0.08277824],
       [-0.00900185],
       [ 0.02182381]]), 0.034652376281363625]
At round 5 accuracy: 0.4247619047619048
At round 5 training accuracy: 0.43387755102040815
At round 5 training loss: 0.41162456359182087
gradient difference: 0.0014310801054006298
At round 5 model param: 
[array([[-2.03372778],
       [ 1.03381045],
       [ 0.76024087],
       [ 0.43474008],
       [ 0.20102883],
       [ 0.0760218 ],
       [-0.01589707],
       [ 0.01687512]]), 0.028535595030656884]
At round 6 accuracy: 0.4247619047619048
At round 6 training accuracy: 0.43387755102040815
At round 6 training loss: 0.4115115361554282
gradient difference: 0.0014430241852412316
At round 6 model param: 
[array([[-2.03407049],
       [ 1.03496842],
       [ 0.75807387],
       [ 0.4333507 ],
       [ 0.19949915],
       [ 0.07755299],
       [-0.01848425],
       [ 0.01561217]]), 0.026716700355921472]
At round 7 accuracy: 0.4247619047619048
At round 7 training accuracy: 0.43387755102040815
At round 7 training loss: 0.41152165617261616
gradient difference: 0.0014420176131967099
At round 7 model param: 
[array([[-2.03687392],
       [ 1.03208519],
       [ 0.75557888],
       [ 0.43343997],
       [ 0.20211733],
       [ 0.07634054],
       [-0.01542624],
       [ 0.0143983 ]]), 0.025252841679113253]
At round 8 accuracy: 0.4247619047619048
At round 8 training accuracy: 0.43387755102040815
At round 8 training loss: 0.4115384519100189
gradient difference: 0.0014462506279784469
At round 8 model param: 
[array([[-2.03096747],
       [ 1.0384684 ],
       [ 0.75798268],
       [ 0.4358422 ],
       [ 0.20465729],
       [ 0.08030183],
       [-0.01191733],
       [ 0.01672278]]), 0.0282101250652756]
At round 9 accuracy: 0.4247619047619048
At round 9 training accuracy: 0.43387755102040815
At round 9 training loss: 0.4115204470498221
gradient difference: 0.0014383330746022915
At round 9 model param: 
[array([[-2.03569099],
       [ 1.03252627],
       [ 0.75808621],
       [ 0.43326111],
       [ 0.20004106],
       [ 0.07630768],
       [-0.01613738],
       [ 0.01574569]]), 0.024835998192429543]
Total training time: 42.94481801986694s
At round 10 accuracy: 0.4247619047619048
At round 10 training accuracy: 0.43387755102040815
At round 10 model param: 
[array([[-2.03569099],
       [ 1.03252627],
       [ 0.75808621],
       [ 0.43326111],
       [ 0.20004106],
       [ 0.07630768],
       [-0.01613738],
       [ 0.01574569]]), 0.024835998192429543]
