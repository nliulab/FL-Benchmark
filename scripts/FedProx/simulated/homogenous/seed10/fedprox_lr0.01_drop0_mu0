Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 10
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.43761904761904763
At round 0 training accuracy: 0.43142857142857144
At round 0 training loss: 0.6931464076042175
gradient difference: 0.001373615237529246
At round 0 model param: 
[array([[-1.78142081],
       [ 0.90709187],
       [ 0.71330151],
       [ 0.34667508],
       [ 0.17437542],
       [ 0.11621341],
       [-0.00327909],
       [-0.01476805]]), 0.02648867600198303]
At round 1 accuracy: 0.43761904761904763
At round 1 training accuracy: 0.43142857142857144
At round 1 training loss: 0.4096240699291229
gradient difference: 0.0004591598685813186
At round 1 model param: 
[array([[-1.96214606],
       [ 1.00364111],
       [ 0.78728641],
       [ 0.38240823],
       [ 0.18856094],
       [ 0.12034863],
       [-0.0028612 ],
       [-0.02105948]]), 0.02541380375623703]
At round 2 accuracy: 0.43761904761904763
At round 2 training accuracy: 0.43142857142857144
At round 2 training loss: 0.4080111341817038
gradient difference: 0.0004407156450053627
At round 2 model param: 
[array([[-1.98976236],
       [ 1.03071415],
       [ 0.80713515],
       [ 0.39893743],
       [ 0.2028355 ],
       [ 0.130664  ],
       [ 0.00476378],
       [-0.01453607]]), 0.03204240729766233]
At round 3 accuracy: 0.43761904761904763
At round 3 training accuracy: 0.43142857142857144
At round 3 training loss: 0.4081149101257324
gradient difference: 0.00044900007564133517
At round 3 model param: 
[array([[-2.00515202],
       [ 1.03036644],
       [ 0.80529548],
       [ 0.39475629],
       [ 0.19548182],
       [ 0.12395148],
       [-0.0026819 ],
       [-0.01926511]]), 0.02568975942475455]
At round 4 accuracy: 0.43761904761904763
At round 4 training accuracy: 0.43142857142857144
At round 4 training loss: 0.4079250139849527
gradient difference: 0.00043937316581957125
At round 4 model param: 
[array([[-2.00338020e+00],
       [ 1.03349638e+00],
       [ 8.07753938e-01],
       [ 3.97160121e-01],
       [ 1.98297609e-01],
       [ 1.29030484e-01],
       [ 1.18044657e-03],
       [-1.61836948e-02]]), 0.027756582679493085]
At round 5 accuracy: 0.43761904761904763
At round 5 training accuracy: 0.43142857142857144
At round 5 training loss: 0.40795983161245075
gradient difference: 0.00044365263020926205
At round 5 model param: 
[array([[-2.00449283],
       [ 1.03290481],
       [ 0.80947757],
       [ 0.39399421],
       [ 0.19822678],
       [ 0.12645913],
       [-0.00284371],
       [-0.01947947]]), 0.0260940675756761]
At round 6 accuracy: 0.43761904761904763
At round 6 training accuracy: 0.43142857142857144
At round 6 training loss: 0.4079284199646541
gradient difference: 0.00044159873853707966
At round 6 model param: 
[array([[-2.00391708e+00],
       [ 1.03093270e+00],
       [ 8.10383107e-01],
       [ 3.98366319e-01],
       [ 2.01128721e-01],
       [ 1.24131196e-01],
       [ 2.52061124e-04],
       [-1.48853162e-02]]), 0.027031496167182922]
At round 7 accuracy: 0.43761904761904763
At round 7 training accuracy: 0.43142857142857144
At round 7 training loss: 0.40795616592679707
gradient difference: 0.0004414715765650688
At round 7 model param: 
[array([[-2.00176491e+00],
       [ 1.03321518e+00],
       [ 8.11685119e-01],
       [ 3.98971298e-01],
       [ 1.99717341e-01],
       [ 1.27756829e-01],
       [-9.11099570e-07],
       [-1.61230043e-02]]), 0.02792677842080593]
At round 8 accuracy: 0.43761904761904763
At round 8 training accuracy: 0.43142857142857144
At round 8 training loss: 0.4079831455435072
gradient difference: 0.00044407506914489765
At round 8 model param: 
[array([[-2.00874557],
       [ 1.02766897],
       [ 0.80509429],
       [ 0.39250473],
       [ 0.19262599],
       [ 0.12151189],
       [-0.00422684],
       [-0.02119327]]), 0.022883295214601924]
At round 9 accuracy: 0.43761904761904763
At round 9 training accuracy: 0.43142857142857144
At round 9 training loss: 0.40796665634427753
gradient difference: 0.0004359166625171088
At round 9 model param: 
[array([[-2.00401299e+00],
       [ 1.03139654e+00],
       [ 8.11183095e-01],
       [ 3.95924151e-01],
       [ 1.97730735e-01],
       [ 1.26487775e-01],
       [-5.72942197e-04],
       [-1.90390069e-02]]), 0.026804091142756597]
Total training time: 39.539456844329834s
At round 10 accuracy: 0.43761904761904763
At round 10 training accuracy: 0.43142857142857144
At round 10 model param: 
[array([[-2.00401299e+00],
       [ 1.03139654e+00],
       [ 8.11183095e-01],
       [ 3.95924151e-01],
       [ 1.97730735e-01],
       [ 1.26487775e-01],
       [-5.72942197e-04],
       [-1.90390069e-02]]), 0.026804091142756597]
