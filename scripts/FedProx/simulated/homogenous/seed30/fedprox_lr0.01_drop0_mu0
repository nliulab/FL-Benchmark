Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 30
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.4338095238095238
At round 0 training accuracy: 0.42448979591836733
At round 0 training loss: 0.6931464076042175
gradient difference: 0.0015947313479618688
At round 0 model param: 
[array([[-1.83042858],
       [ 0.87093218],
       [ 0.73099196],
       [ 0.38678526],
       [ 0.16071427],
       [ 0.07495581],
       [-0.02891653],
       [ 0.00331092]]), 0.13222998114568846]
At round 1 accuracy: 0.4338095238095238
At round 1 training accuracy: 0.42448979591836733
At round 1 training loss: 0.40703662378447397
gradient difference: 0.0008340202085644175
At round 1 model param: 
[array([[-2.02620789e+00],
       [ 9.62304149e-01],
       [ 8.03677559e-01],
       [ 4.20774881e-01],
       [ 1.72797546e-01],
       [ 7.25645569e-02],
       [-2.89314996e-02],
       [ 2.76455390e-04]]), 0.17796999608565653]
At round 2 accuracy: 0.4338095238095238
At round 2 training accuracy: 0.42448979591836733
At round 2 training loss: 0.4056148614202227
gradient difference: 0.0008438006450414262
At round 2 model param: 
[array([[-2.06803496e+00],
       [ 9.82077241e-01],
       [ 8.19593668e-01],
       [ 4.28130661e-01],
       [ 1.74131994e-01],
       [ 7.37764678e-02],
       [-3.06469608e-02],
       [ 1.49286858e-03]]), 0.2035884918378932]
At round 3 accuracy: 0.4338095238095238
At round 3 training accuracy: 0.42448979591836733
At round 3 training loss: 0.4056564356599535
gradient difference: 0.00084516126532152
At round 3 model param: 
[array([[-2.08380202],
       [ 0.98356858],
       [ 0.81745401],
       [ 0.4217384 ],
       [ 0.17075451],
       [ 0.067679  ],
       [-0.03623357],
       [-0.00520998]]), 0.21393737941980362]
At round 4 accuracy: 0.4338095238095238
At round 4 training accuracy: 0.42448979591836733
At round 4 training loss: 0.4056859016418457
gradient difference: 0.0008535908210409527
At round 4 model param: 
[array([[-2.08484585],
       [ 0.98111127],
       [ 0.82039474],
       [ 0.42455027],
       [ 0.17214254],
       [ 0.06812718],
       [-0.03718819],
       [-0.00466802]]), 0.22298811376094818]
At round 5 accuracy: 0.4338095238095238
At round 5 training accuracy: 0.42448979591836733
At round 5 training loss: 0.40567188603537424
gradient difference: 0.0008505914049599687
At round 5 model param: 
[array([[-2.08456121e+00],
       [ 9.84209844e-01],
       [ 8.20670170e-01],
       [ 4.24570207e-01],
       [ 1.77240797e-01],
       [ 7.18448370e-02],
       [-3.73937716e-02],
       [-1.73546800e-03]]), 0.22957315828119004]
At round 6 accuracy: 0.4338095238095238
At round 6 training accuracy: 0.42448979591836733
At round 6 training loss: 0.40569158111299786
gradient difference: 0.0008488806206355605
At round 6 model param: 
[array([[-2.08183312e+00],
       [ 9.85378708e-01],
       [ 8.25347117e-01],
       [ 4.26983178e-01],
       [ 1.76752114e-01],
       [ 7.46006380e-02],
       [-3.24559804e-02],
       [ 3.42522349e-04]]), 0.23488965736968176]
At round 7 accuracy: 0.4338095238095238
At round 7 training accuracy: 0.42448979591836733
At round 7 training loss: 0.4057859991277967
gradient difference: 0.0008441760670975873
At round 7 model param: 
[array([[-2.08795421],
       [ 0.98187719],
       [ 0.81862466],
       [ 0.42377056],
       [ 0.16950046],
       [ 0.06754375],
       [-0.0380207 ],
       [-0.00526217]]), 0.2320563229066985]
At round 8 accuracy: 0.4338095238095238
At round 8 training accuracy: 0.42448979591836733
At round 8 training loss: 0.4056884433541979
gradient difference: 0.0008525353555022836
At round 8 model param: 
[array([[-2.08158122e+00],
       [ 9.87699968e-01],
       [ 8.21151640e-01],
       [ 4.28441231e-01],
       [ 1.74613293e-01],
       [ 6.94517719e-02],
       [-3.22554445e-02],
       [-4.86504819e-04]]), 0.23705199999468668]
At round 9 accuracy: 0.4338095238095238
At round 9 training accuracy: 0.42448979591836733
At round 9 training loss: 0.4057580658367702
gradient difference: 0.0008443618539440007
At round 9 model param: 
[array([[-2.08952672],
       [ 0.98044808],
       [ 0.81623495],
       [ 0.42369459],
       [ 0.16852769],
       [ 0.0638325 ],
       [-0.04353655],
       [-0.00828127]]), 0.23095441716057913]
Total training time: 37.40537881851196s
At round 10 accuracy: 0.4338095238095238
At round 10 training accuracy: 0.42448979591836733
At round 10 model param: 
[array([[-2.08952672],
       [ 0.98044808],
       [ 0.81623495],
       [ 0.42369459],
       [ 0.16852769],
       [ 0.0638325 ],
       [-0.04353655],
       [-0.00828127]]), 0.23095441716057913]
