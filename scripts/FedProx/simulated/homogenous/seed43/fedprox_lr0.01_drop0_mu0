Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 43
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.43857142857142856
At round 0 training accuracy: 0.4387755102040816
At round 0 training loss: 0.6931464076042175
gradient difference: 0.0033728187429840693
At round 0 model param: 
[array([[-1.83900137e+00],
       [ 9.31022993e-01],
       [ 7.07017171e-01],
       [ 3.63273080e-01],
       [ 2.16170552e-01],
       [ 3.03167413e-02],
       [-1.62374388e-03],
       [ 5.59885161e-03]]), 0.04979519732296467]
At round 1 accuracy: 0.43857142857142856
At round 1 training accuracy: 0.4387755102040816
At round 1 training loss: 0.3952208161354065
gradient difference: 0.0008616755617586657
At round 1 model param: 
[array([[-2.04751509],
       [ 1.02192809],
       [ 0.77404353],
       [ 0.39601997],
       [ 0.22059027],
       [ 0.01030662],
       [-0.00743937],
       [-0.00444089]]), 0.05222351769251483]
At round 2 accuracy: 0.43857142857142856
At round 2 training accuracy: 0.4387755102040816
At round 2 training loss: 0.3933721312454769
gradient difference: 0.0008551035067513764
At round 2 model param: 
[array([[-2.08804853],
       [ 1.04280838],
       [ 0.79510489],
       [ 0.40938113],
       [ 0.22657447],
       [ 0.01446341],
       [-0.00324292],
       [-0.0027177 ]]), 0.060120509238913655]
At round 3 accuracy: 0.43857142857142856
At round 3 training accuracy: 0.4387755102040816
At round 3 training loss: 0.3931095004081726
gradient difference: 0.0008426762512017909
At round 3 model param: 
[array([[-2.09455984e+00],
       [ 1.05173446e+00],
       [ 8.00045763e-01],
       [ 4.11035197e-01],
       [ 2.33651270e-01],
       [ 1.40081671e-02],
       [-1.48152613e-03],
       [-9.10954816e-04]]), 0.06538071011060051]
At round 4 accuracy: 0.43857142857142856
At round 4 training accuracy: 0.4387755102040816
At round 4 training loss: 0.3930761473519461
gradient difference: 0.0008368612255069272
At round 4 model param: 
[array([[-2.10052752],
       [ 1.04758804],
       [ 0.80182651],
       [ 0.41074803],
       [ 0.22966723],
       [ 0.01265131],
       [-0.00527545],
       [-0.00436183]]), 0.06504958202796322]
At round 5 accuracy: 0.43857142857142856
At round 5 training accuracy: 0.4387755102040816
At round 5 training loss: 0.3931022712162563
gradient difference: 0.0008397956370463168
At round 5 model param: 
[array([[-2.09799222e+00],
       [ 1.05292237e+00],
       [ 8.01037014e-01],
       [ 4.13449317e-01],
       [ 2.30646885e-01],
       [ 1.66152045e-02],
       [-1.82310332e-03],
       [-3.56940287e-04]]), 0.06832948526633638]
At round 6 accuracy: 0.43857142857142856
At round 6 training accuracy: 0.4387755102040816
At round 6 training loss: 0.39307448267936707
gradient difference: 0.0008353643212080684
At round 6 model param: 
[array([[-2.09941629e+00],
       [ 1.05143014e+00],
       [ 7.98208807e-01],
       [ 4.09049149e-01],
       [ 2.33006113e-01],
       [ 1.36557338e-02],
       [-7.47803599e-04],
       [-5.52970490e-03]]), 0.06710890786988395]
At round 7 accuracy: 0.43857142857142856
At round 7 training accuracy: 0.4387755102040816
At round 7 training loss: 0.3930834233760834
gradient difference: 0.0008399001291575743
At round 7 model param: 
[array([[-2.09250457],
       [ 1.0583717 ],
       [ 0.80598321],
       [ 0.41834646],
       [ 0.23489988],
       [ 0.01831951],
       [ 0.00318876],
       [ 0.00265627]]), 0.0730505728029779]
At round 8 accuracy: 0.43857142857142856
At round 8 training accuracy: 0.4387755102040816
At round 8 training loss: 0.39320904442242216
gradient difference: 0.0008273518656114585
At round 8 model param: 
[array([[-2.10074122],
       [ 1.04983924],
       [ 0.79963291],
       [ 0.41048406],
       [ 0.22773339],
       [ 0.01215839],
       [-0.00525343],
       [-0.00417665]]), 0.06628990891788687]
At round 9 accuracy: 0.43857142857142856
At round 9 training accuracy: 0.4387755102040816
At round 9 training loss: 0.3931057069982801
gradient difference: 0.0008409199415968166
At round 9 model param: 
[array([[-2.09631930e+00],
       [ 1.05381026e+00],
       [ 8.03461679e-01],
       [ 4.15979654e-01],
       [ 2.34373672e-01],
       [ 1.87998302e-02],
       [ 3.11724895e-03],
       [-3.04361539e-04]]), 0.07181867025792599]
Total training time: 40.762540102005005s
At round 10 accuracy: 0.43857142857142856
At round 10 training accuracy: 0.4387755102040816
At round 10 model param: 
[array([[-2.09631930e+00],
       [ 1.05381026e+00],
       [ 8.03461679e-01],
       [ 4.15979654e-01],
       [ 2.34373672e-01],
       [ 1.87998302e-02],
       [ 3.11724895e-03],
       [-3.04361539e-04]]), 0.07181867025792599]
