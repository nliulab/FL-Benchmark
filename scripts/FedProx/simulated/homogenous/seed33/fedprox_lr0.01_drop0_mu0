Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 33
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.4438095238095238
At round 0 training accuracy: 0.4197959183673469
At round 0 training loss: 0.6931464076042175
gradient difference: 0.0024581876605471883
At round 0 model param: 
[array([[-1.78678712],
       [ 0.89797074],
       [ 0.61589475],
       [ 0.36192148],
       [ 0.19639982],
       [ 0.07967561],
       [ 0.08411007],
       [ 0.02555871]]), 0.03563176574451583]
At round 1 accuracy: 0.4438095238095238
At round 1 training accuracy: 0.4197959183673469
At round 1 training loss: 0.4055184636797224
gradient difference: 0.0012738016216445273
At round 1 model param: 
[array([[-1.96923321],
       [ 0.99126894],
       [ 0.67589973],
       [ 0.39851984],
       [ 0.2206088 ],
       [ 0.07985138],
       [ 0.08335662],
       [ 0.02417347]]), 0.03475402880992208]
At round 2 accuracy: 0.4438095238095238
At round 2 training accuracy: 0.4197959183673469
At round 2 training loss: 0.403705541576658
gradient difference: 0.0012677644979246878
At round 2 model param: 
[array([[-1.99711101],
       [ 1.01648196],
       [ 0.69591692],
       [ 0.41295091],
       [ 0.23083345],
       [ 0.08384845],
       [ 0.09165575],
       [ 0.02737295]]), 0.04016670478241784]
At round 3 accuracy: 0.4438095238095238
At round 3 training accuracy: 0.4197959183673469
At round 3 training loss: 0.40373869027410236
gradient difference: 0.0012623468942397221
At round 3 model param: 
[array([[-2.0090894 ],
       [ 1.01582805],
       [ 0.69753996],
       [ 0.41280715],
       [ 0.22869854],
       [ 0.0789625 ],
       [ 0.08751836],
       [ 0.02682663]]), 0.036075442497219355]
At round 4 accuracy: 0.4438095238095238
At round 4 training accuracy: 0.4197959183673469
At round 4 training loss: 0.40361036147390095
gradient difference: 0.001263283770337406
At round 4 model param: 
[array([[-2.01205025],
       [ 1.01499828],
       [ 0.69461828],
       [ 0.41132752],
       [ 0.22782388],
       [ 0.07764439],
       [ 0.08580724],
       [ 0.02201952]]), 0.03362204560211727]
At round 5 accuracy: 0.4438095238095238
At round 5 training accuracy: 0.4197959183673469
At round 5 training loss: 0.4035931485039847
gradient difference: 0.0012645646873374562
At round 5 model param: 
[array([[-2.00853474],
       [ 1.02015345],
       [ 0.69558134],
       [ 0.41688409],
       [ 0.23240558],
       [ 0.08235622],
       [ 0.09082437],
       [ 0.02718723]]), 0.0373953181718077]
At round 6 accuracy: 0.4438095238095238
At round 6 training accuracy: 0.4197959183673469
At round 6 training loss: 0.4036781745297568
gradient difference: 0.0012643227073326493
At round 6 model param: 
[array([[-2.0098473 ],
       [ 1.01939668],
       [ 0.6953655 ],
       [ 0.41302033],
       [ 0.22916175],
       [ 0.08333462],
       [ 0.08963327],
       [ 0.02565686]]), 0.03552516975573131]
At round 7 accuracy: 0.4438095238095238
At round 7 training accuracy: 0.4197959183673469
At round 7 training loss: 0.4036248198577336
gradient difference: 0.001263614887169157
At round 7 model param: 
[array([[-2.01724182],
       [ 1.01250906],
       [ 0.6887028 ],
       [ 0.40916785],
       [ 0.22588049],
       [ 0.07249566],
       [ 0.08573561],
       [ 0.02086856]]), 0.02982434577175549]
At round 8 accuracy: 0.4438095238095238
At round 8 training accuracy: 0.4197959183673469
At round 8 training loss: 0.40366765005247934
gradient difference: 0.0012689600875539326
At round 8 model param: 
[array([[-2.01012887],
       [ 1.01878394],
       [ 0.69693977],
       [ 0.41098835],
       [ 0.23009387],
       [ 0.0807814 ],
       [ 0.08635905],
       [ 0.02303097]]), 0.03432961979082653]
At round 9 accuracy: 0.4438095238095238
At round 9 training accuracy: 0.4197959183673469
At round 9 training loss: 0.4035989003522055
gradient difference: 0.0012633074879123674
At round 9 model param: 
[array([[-2.01342855],
       [ 1.01354279],
       [ 0.69542399],
       [ 0.40800532],
       [ 0.22503081],
       [ 0.07880005],
       [ 0.08907677],
       [ 0.02258632]]), 0.032811286726168225]
Total training time: 47.634072065353394s
At round 10 accuracy: 0.4438095238095238
At round 10 training accuracy: 0.4197959183673469
At round 10 model param: 
[array([[-2.01342855],
       [ 1.01354279],
       [ 0.69542399],
       [ 0.40800532],
       [ 0.22503081],
       [ 0.07880005],
       [ 0.08907677],
       [ 0.02258632]]), 0.032811286726168225]
