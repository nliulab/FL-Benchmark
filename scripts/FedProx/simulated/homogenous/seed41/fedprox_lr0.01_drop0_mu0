Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 41
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.43666666666666665
At round 0 training accuracy: 0.42142857142857143
At round 0 training loss: 0.6931464076042175
gradient difference: 0.0027284957733195044
At round 0 model param: 
[array([[-1.8335582 ],
       [ 0.87569069],
       [ 0.6938163 ],
       [ 0.34550167],
       [ 0.2504168 ],
       [ 0.10163619],
       [ 0.01586993],
       [ 0.03528101]]), 0.041722796857357025]
At round 1 accuracy: 0.43666666666666665
At round 1 training accuracy: 0.42142857142857143
At round 1 training loss: 0.40097713470458984
gradient difference: 0.0010728559581115347
At round 1 model param: 
[array([[-2.03987745],
       [ 0.96312807],
       [ 0.76138629],
       [ 0.37190094],
       [ 0.27569113],
       [ 0.11097383],
       [ 0.00767179],
       [ 0.03112513]]), 0.025017801406128064]
At round 2 accuracy: 0.43666666666666665
At round 2 training accuracy: 0.42142857142857143
At round 2 training loss: 0.3991834265845163
gradient difference: 0.0010315813716672484
At round 2 model param: 
[array([[-2.07199352],
       [ 0.99369083],
       [ 0.78358746],
       [ 0.39069006],
       [ 0.2947611 ],
       [ 0.11968437],
       [ 0.01285239],
       [ 0.04011541]]), 0.02287626831925341]
At round 3 accuracy: 0.43666666666666665
At round 3 training accuracy: 0.42142857142857143
At round 3 training loss: 0.39893833228519987
gradient difference: 0.0010826022644363031
At round 3 model param: 
[array([[-2.08667799],
       [ 0.99228007],
       [ 0.78207573],
       [ 0.38719443],
       [ 0.29006998],
       [ 0.11980602],
       [ 0.01211154],
       [ 0.04054936]]), 0.012529186638338225]
At round 4 accuracy: 0.43666666666666665
At round 4 training accuracy: 0.42142857142857143
At round 4 training loss: 0.3989166830267225
gradient difference: 0.0010604400713141143
At round 4 model param: 
[array([[-2.08678123],
       [ 0.9957794 ],
       [ 0.78496677],
       [ 0.39215191],
       [ 0.29556548],
       [ 0.12098275],
       [ 0.01356028],
       [ 0.0380986 ]]), 0.010137598030269146]
At round 5 accuracy: 0.43666666666666665
At round 5 training accuracy: 0.42142857142857143
At round 5 training loss: 0.39889797994068693
gradient difference: 0.0010709204274471187
At round 5 model param: 
[array([[-2.0832665 ],
       [ 0.99684255],
       [ 0.79135735],
       [ 0.39472254],
       [ 0.29888971],
       [ 0.12687903],
       [ 0.01854686],
       [ 0.03940643]]), 0.010630222569618906]
At round 6 accuracy: 0.43666666666666665
At round 6 training accuracy: 0.42142857142857143
At round 6 training loss: 0.3989609479904175
gradient difference: 0.0010899981121906333
At round 6 model param: 
[array([[-2.08219831],
       [ 1.00151336],
       [ 0.78686389],
       [ 0.39422689],
       [ 0.29819578],
       [ 0.1245625 ],
       [ 0.01763879],
       [ 0.0406459 ]]), 0.009172181199703897]
At round 7 accuracy: 0.43666666666666665
At round 7 training accuracy: 0.42142857142857143
At round 7 training loss: 0.398945438010352
gradient difference: 0.001086953147873546
At round 7 model param: 
[array([[-2.08599295],
       [ 0.99697719],
       [ 0.78657365],
       [ 0.38798015],
       [ 0.2991613 ],
       [ 0.12339989],
       [ 0.01301337],
       [ 0.04205164]]), 0.006401041788714272]
At round 8 accuracy: 0.43666666666666665
At round 8 training accuracy: 0.42142857142857143
At round 8 training loss: 0.398896051304681
gradient difference: 0.001070721062781604
At round 8 model param: 
[array([[-2.09074504],
       [ 0.99291692],
       [ 0.78226516],
       [ 0.38630466],
       [ 0.29010289],
       [ 0.1174014 ],
       [ 0.00973356],
       [ 0.03703957]]), 0.0012864294861044203]
At round 9 accuracy: 0.43666666666666665
At round 9 training accuracy: 0.42142857142857143
At round 9 training loss: 0.3990033609526498
gradient difference: 0.0010471101914205706
At round 9 model param: 
[array([[-2.09287756],
       [ 0.9889508 ],
       [ 0.78288075],
       [ 0.38338042],
       [ 0.28968019],
       [ 0.11561973],
       [ 0.00800335],
       [ 0.03380001]]), -0.0005009169025080544]
Total training time: 35.06642818450928s
At round 10 accuracy: 0.43666666666666665
At round 10 training accuracy: 0.42142857142857143
At round 10 model param: 
[array([[-2.09287756],
       [ 0.9889508 ],
       [ 0.78288075],
       [ 0.38338042],
       [ 0.28968019],
       [ 0.11561973],
       [ 0.00800335],
       [ 0.03380001]]), -0.0005009169025080544]
