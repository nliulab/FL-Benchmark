Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 34
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.4461904761904762
At round 0 training accuracy: 0.4316326530612245
At round 0 training loss: 0.6931464076042175
gradient difference: 0.001078458610021057
At round 0 model param: 
[array([[-1.73230645],
       [ 0.93241341],
       [ 0.74238694],
       [ 0.34608005],
       [ 0.12991916],
       [ 0.04657289],
       [ 0.03336804],
       [ 0.02087083]]), -0.06119130412116647]
At round 1 accuracy: 0.4461904761904762
At round 1 training accuracy: 0.4316326530612245
At round 1 training loss: 0.41071377055985586
gradient difference: 0.0006563560370626943
At round 1 model param: 
[array([[-1.9121131 ],
       [ 1.02123451],
       [ 0.81568464],
       [ 0.38116289],
       [ 0.14277517],
       [ 0.0538662 ],
       [ 0.03452   ],
       [ 0.02495135]]), -0.10878232600433486]
At round 2 accuracy: 0.4461904761904762
At round 2 training accuracy: 0.4316326530612245
At round 2 training loss: 0.40914628335407804
gradient difference: 0.0006601736174452931
At round 2 model param: 
[array([[-1.9454494 ],
       [ 1.04185063],
       [ 0.82963243],
       [ 0.38988209],
       [ 0.15119735],
       [ 0.05657839],
       [ 0.0368423 ],
       [ 0.02926958]]), -0.1357199719973973]
At round 3 accuracy: 0.4461904761904762
At round 3 training accuracy: 0.4316326530612245
At round 3 training loss: 0.40909951499530245
gradient difference: 0.0006634460981028366
At round 3 model param: 
[array([[-1.95212913],
       [ 1.0438133 ],
       [ 0.83682498],
       [ 0.3915617 ],
       [ 0.14990935],
       [ 0.05890244],
       [ 0.03764004],
       [ 0.0318317 ]]), -0.15295184829405375]
At round 4 accuracy: 0.4461904761904762
At round 4 training accuracy: 0.4316326530612245
At round 4 training loss: 0.40911937611443655
gradient difference: 0.0006641060824325548
At round 4 model param: 
[array([[-1.93525733],
       [ 1.06291548],
       [ 0.8526436 ],
       [ 0.40838951],
       [ 0.16341219],
       [ 0.07244502],
       [ 0.05323798],
       [ 0.04443375]]), -0.14955079874822072]
At round 5 accuracy: 0.4461904761904762
At round 5 training accuracy: 0.4316326530612245
At round 5 training loss: 0.4097952629838671
gradient difference: 0.0006504220217360641
At round 5 model param: 
[array([[-1.94599734],
       [ 1.05325747],
       [ 0.84316746],
       [ 0.39894939],
       [ 0.15642429],
       [ 0.06428025],
       [ 0.04408936],
       [ 0.034113  ]]), -0.16388284521443502]
At round 6 accuracy: 0.4461904761904762
At round 6 training accuracy: 0.4316326530612245
At round 6 training loss: 0.4090785469327654
gradient difference: 0.0006602478677315726
At round 6 model param: 
[array([[-1.94617168],
       [ 1.05272929],
       [ 0.8422947 ],
       [ 0.3987395 ],
       [ 0.15571002],
       [ 0.06716374],
       [ 0.04625488],
       [ 0.03542073]]), -0.16699097199099405]
At round 7 accuracy: 0.4461904761904762
At round 7 training accuracy: 0.4316326530612245
At round 7 training loss: 0.4090789386204311
gradient difference: 0.0006609082863728704
At round 7 model param: 
[array([[-1.9470435 ],
       [ 1.05325603],
       [ 0.84356162],
       [ 0.39685914],
       [ 0.15511362],
       [ 0.06452571],
       [ 0.04502268],
       [ 0.03618916]]), -0.16985689103603363]
At round 8 accuracy: 0.4461904761904762
At round 8 training accuracy: 0.4316326530612245
At round 8 training loss: 0.409068409885679
gradient difference: 0.0006613353899534235
At round 8 model param: 
[array([[-1.95123039],
       [ 1.04797499],
       [ 0.83852768],
       [ 0.39253198],
       [ 0.15185387],
       [ 0.06010767],
       [ 0.03596793],
       [ 0.0318991 ]]), -0.1751739595617567]
At round 9 accuracy: 0.4461904761904762
At round 9 training accuracy: 0.4316326530612245
At round 9 training loss: 0.409172990492412
gradient difference: 0.0006666949739889492
At round 9 model param: 
[array([[-1.94488345],
       [ 1.05353241],
       [ 0.84455259],
       [ 0.39996993],
       [ 0.15617265],
       [ 0.06762643],
       [ 0.04536485],
       [ 0.03479316]]), -0.17044058867863246]
Total training time: 42.80204129219055s
At round 10 accuracy: 0.4461904761904762
At round 10 training accuracy: 0.4316326530612245
At round 10 model param: 
[array([[-1.94488345],
       [ 1.05353241],
       [ 0.84455259],
       [ 0.39996993],
       [ 0.15617265],
       [ 0.06762643],
       [ 0.04536485],
       [ 0.03479316]]), -0.17044058867863246]
