Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 4
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.4166666666666667
At round 0 training accuracy: 0.41775510204081634
At round 0 training loss: 0.6931464076042175
gradient difference: 0.0015656184826589776
At round 0 model param: 
[array([[-1.87586095],
       [ 0.83230228],
       [ 0.71417893],
       [ 0.33733534],
       [ 0.1988291 ],
       [ 0.10924754],
       [ 0.0214172 ],
       [ 0.0379786 ]]), 0.11943739546196801]
At round 1 accuracy: 0.4166666666666667
At round 1 training accuracy: 0.41775510204081634
At round 1 training loss: 0.3954783209732601
gradient difference: 0.0009589706189026339
At round 1 model param: 
[array([[-2.09140801],
       [ 0.91474013],
       [ 0.78584166],
       [ 0.36379123],
       [ 0.20915444],
       [ 0.12119438],
       [ 0.01405596],
       [ 0.03585835]]), 0.1566124153988702]
At round 2 accuracy: 0.4166666666666667
At round 2 training accuracy: 0.41775510204081634
At round 2 training loss: 0.3935319014957973
gradient difference: 0.0009677267895543519
At round 2 model param: 
[array([[-2.12875748],
       [ 0.94087295],
       [ 0.80857009],
       [ 0.38118582],
       [ 0.22013855],
       [ 0.13252193],
       [ 0.02105458],
       [ 0.04180752]]), 0.188333660364151]
At round 3 accuracy: 0.4166666666666667
At round 3 training accuracy: 0.41775510204081634
At round 3 training loss: 0.3935007836137499
gradient difference: 0.0009708547745053111
At round 3 model param: 
[array([[-2.15257982],
       [ 0.93376517],
       [ 0.80242566],
       [ 0.36905918],
       [ 0.20744064],
       [ 0.12386196],
       [ 0.01185055],
       [ 0.03172673]]), 0.19251573724406107]
At round 4 accuracy: 0.4166666666666667
At round 4 training accuracy: 0.41775510204081634
At round 4 training loss: 0.3934203599180494
gradient difference: 0.0009738047066295448
At round 4 model param: 
[array([[-2.15155445],
       [ 0.93845378],
       [ 0.80551332],
       [ 0.37551598],
       [ 0.21292707],
       [ 0.12615553],
       [ 0.01344963],
       [ 0.03866583]]), 0.2047560257571084]
At round 5 accuracy: 0.4166666666666667
At round 5 training accuracy: 0.41775510204081634
At round 5 training loss: 0.39330892477716717
gradient difference: 0.0009722203244520363
At round 5 model param: 
[array([[-2.15283843],
       [ 0.93717593],
       [ 0.80653978],
       [ 0.37284634],
       [ 0.21280927],
       [ 0.1244633 ],
       [ 0.01198823],
       [ 0.03340106]]), 0.20919061771460942]
At round 6 accuracy: 0.4166666666666667
At round 6 training accuracy: 0.41775510204081634
At round 6 training loss: 0.3933072303022657
gradient difference: 0.0009738310526364725
At round 6 model param: 
[array([[-2.15209719],
       [ 0.9393156 ],
       [ 0.8062397 ],
       [ 0.37304105],
       [ 0.21455932],
       [ 0.12310155],
       [ 0.01531943],
       [ 0.03550376]]), 0.2130730343716485]
At round 7 accuracy: 0.4166666666666667
At round 7 training accuracy: 0.41775510204081634
At round 7 training loss: 0.39330558265958515
gradient difference: 0.0009727546591598433
At round 7 model param: 
[array([[-2.15997815],
       [ 0.93084971],
       [ 0.80110634],
       [ 0.36930073],
       [ 0.20656003],
       [ 0.11846839],
       [ 0.00822898],
       [ 0.02788251]]), 0.20936455258301326]
At round 8 accuracy: 0.4166666666666667
At round 8 training accuracy: 0.41775510204081634
At round 8 training loss: 0.3934751663889204
gradient difference: 0.0009775113919633096
At round 8 model param: 
[array([[-2.15558294],
       [ 0.93434983],
       [ 0.80510306],
       [ 0.37280459],
       [ 0.21152371],
       [ 0.12425654],
       [ 0.01303924],
       [ 0.03281135]]), 0.21447249395506723]
At round 9 accuracy: 0.4166666666666667
At round 9 training accuracy: 0.41775510204081634
At round 9 training loss: 0.39330764753477915
gradient difference: 0.0009757329721395828
At round 9 model param: 
[array([[-2.15134522],
       [ 0.93912684],
       [ 0.8062892 ],
       [ 0.37621358],
       [ 0.21078811],
       [ 0.12712442],
       [ 0.01285803],
       [ 0.03629822]]), 0.21688941546848842]
Total training time: 46.174087047576904s
At round 10 accuracy: 0.4166666666666667
At round 10 training accuracy: 0.41775510204081634
At round 10 model param: 
[array([[-2.15134522],
       [ 0.93912684],
       [ 0.8062892 ],
       [ 0.37621358],
       [ 0.21078811],
       [ 0.12712442],
       [ 0.01285803],
       [ 0.03629822]]), 0.21688941546848842]
