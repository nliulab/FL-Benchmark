Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 2
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.43476190476190474
At round 0 training accuracy: 0.4246938775510204
At round 0 training loss: 0.6931464076042175
gradient difference: 0.0038626777650924474
At round 0 model param: 
[array([[-1.78464203],
       [ 0.89967439],
       [ 0.70128672],
       [ 0.3838701 ],
       [ 0.17199517],
       [ 0.09242499],
       [-0.01381759],
       [ 0.04561463]]), -0.02256633128438677]
At round 1 accuracy: 0.43476190476190474
At round 1 training accuracy: 0.4246938775510204
At round 1 training loss: 0.4052650715623583
gradient difference: 0.00027469659706144
At round 1 model param: 
[array([[-1.97146118],
       [ 0.99722884],
       [ 0.79211452],
       [ 0.43242724],
       [ 0.19705035],
       [ 0.11388451],
       [-0.00268336],
       [ 0.05334933]]), -0.05594370221453054]
At round 2 accuracy: 0.43476190476190474
At round 2 training accuracy: 0.4246938775510204
At round 2 training loss: 0.4034578672477177
gradient difference: 0.0002590714441853633
At round 2 model param: 
[array([[-2.018552  ],
       [ 1.01249983],
       [ 0.80296628],
       [ 0.43800925],
       [ 0.19157342],
       [ 0.1121761 ],
       [-0.00944089],
       [ 0.04726729]]), -0.08851834599460874]
At round 3 accuracy: 0.43476190476190474
At round 3 training accuracy: 0.4246938775510204
At round 3 training loss: 0.40297920789037434
gradient difference: 0.0002388544921634535
At round 3 model param: 
[array([[-2.02133908],
       [ 1.02484613],
       [ 0.81746406],
       [ 0.44300603],
       [ 0.19994987],
       [ 0.11612787],
       [-0.00479521],
       [ 0.05271843]]), -0.09971538079636437]
At round 4 accuracy: 0.43476190476190474
At round 4 training accuracy: 0.4246938775510204
At round 4 training loss: 0.4030521128858839
gradient difference: 0.0002451769417780141
At round 4 model param: 
[array([[-2.02392289],
       [ 1.02436592],
       [ 0.81981909],
       [ 0.44452448],
       [ 0.19822401],
       [ 0.11932067],
       [-0.00676426],
       [ 0.05123874]]), -0.10954637293304716]
At round 5 accuracy: 0.43476190476190474
At round 5 training accuracy: 0.4246938775510204
At round 5 training loss: 0.4030002014977591
gradient difference: 0.00024253450518905168
At round 5 model param: 
[array([[-2.02012508e+00],
       [ 1.02913184e+00],
       [ 8.21685961e-01],
       [ 4.50027198e-01],
       [ 2.04391226e-01],
       [ 1.20950956e-01],
       [ 1.13889209e-03],
       [ 5.51599299e-02]]), -0.11166177902902875]
At round 6 accuracy: 0.43476190476190474
At round 6 training accuracy: 0.4246938775510204
At round 6 training loss: 0.4032069444656372
gradient difference: 0.0002491411583801027
At round 6 model param: 
[array([[-2.02917688],
       [ 1.02211414],
       [ 0.8150382 ],
       [ 0.44084574],
       [ 0.19467391],
       [ 0.11528507],
       [-0.00638988],
       [ 0.04866395]]), -0.12256693627153124]
At round 7 accuracy: 0.43476190476190474
At round 7 training accuracy: 0.4246938775510204
At round 7 training loss: 0.4029590146882193
gradient difference: 0.00023566568923752074
At round 7 model param: 
[array([[-2.03004016],
       [ 1.02243582],
       [ 0.81440047],
       [ 0.44053672],
       [ 0.19886381],
       [ 0.1133395 ],
       [-0.00892216],
       [ 0.05175186]]), -0.12491197032587868]
At round 8 accuracy: 0.43476190476190474
At round 8 training accuracy: 0.4246938775510204
At round 8 training loss: 0.4029620332377298
gradient difference: 0.00023524420684464134
At round 8 model param: 
[array([[-2.02815478],
       [ 1.02187954],
       [ 0.81673103],
       [ 0.44441221],
       [ 0.19609204],
       [ 0.11670131],
       [-0.00380606],
       [ 0.05206762]]), -0.12397268840244838]
At round 9 accuracy: 0.43476190476190474
At round 9 training accuracy: 0.4246938775510204
At round 9 training loss: 0.4029490905148642
gradient difference: 0.00023844973061204586
At round 9 model param: 
[array([[-2.02572356],
       [ 1.02524241],
       [ 0.81763051],
       [ 0.44333609],
       [ 0.19908175],
       [ 0.11853128],
       [-0.00505865],
       [ 0.05284858]]), -0.12343075019972664]
Total training time: 42.259992837905884s
At round 10 accuracy: 0.43476190476190474
At round 10 training accuracy: 0.4246938775510204
At round 10 model param: 
[array([[-2.02572356],
       [ 1.02524241],
       [ 0.81763051],
       [ 0.44333609],
       [ 0.19908175],
       [ 0.11853128],
       [-0.00505865],
       [ 0.05284858]]), -0.12343075019972664]
