Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 19
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.4195238095238095
At round 0 training accuracy: 0.4287755102040816
At round 0 training loss: 0.6931464076042175
gradient difference: 0.004002767985098381
At round 0 model param: 
[array([[-1.78485193],
       [ 0.95666555],
       [ 0.72590984],
       [ 0.39641864],
       [ 0.14873693],
       [ 0.0334037 ],
       [-0.01340378],
       [ 0.01364083]]), 0.03010514165673937]
At round 1 accuracy: 0.4195238095238095
At round 1 training accuracy: 0.4287755102040816
At round 1 training loss: 0.40918731689453125
gradient difference: 0.003572433018903275
At round 1 model param: 
[array([[-1.98998087],
       [ 1.0539396 ],
       [ 0.7917652 ],
       [ 0.43376537],
       [ 0.15973933],
       [ 0.0251032 ],
       [-0.01979234],
       [ 0.00386675]]), 0.01355866136561547]
At round 2 accuracy: 0.4195238095238095
At round 2 training accuracy: 0.4287755102040816
At round 2 training loss: 0.4075728441987719
gradient difference: 0.0035324195715134587
At round 2 model param: 
[array([[-2.02604556],
       [ 1.08176013],
       [ 0.81150184],
       [ 0.44641475],
       [ 0.16574734],
       [ 0.03452004],
       [-0.01534024],
       [ 0.00737955]]), 0.010082105174660683]
At round 3 accuracy: 0.4195238095238095
At round 3 training accuracy: 0.4287755102040816
At round 3 training loss: 0.407437264919281
gradient difference: 0.003536756881753936
At round 3 model param: 
[array([[-2.03204472],
       [ 1.09064591],
       [ 0.81854938],
       [ 0.4518348 ],
       [ 0.1721106 ],
       [ 0.03293475],
       [-0.01220601],
       [ 0.00659998]]), 0.007032578944095543]
At round 4 accuracy: 0.4195238095238095
At round 4 training accuracy: 0.4287755102040816
At round 4 training loss: 0.4074707201548985
gradient difference: 0.003536913011296661
At round 4 model param: 
[array([[-2.03558135],
       [ 1.09191392],
       [ 0.82170885],
       [ 0.45125431],
       [ 0.16697286],
       [ 0.03370928],
       [-0.01264721],
       [ 0.01202306]]), 0.00401510763913393]
At round 5 accuracy: 0.4195238095238095
At round 5 training accuracy: 0.4287755102040816
At round 5 training loss: 0.4074686425072806
gradient difference: 0.003536620052892793
At round 5 model param: 
[array([[-2.04074771],
       [ 1.08773547],
       [ 0.81687199],
       [ 0.4491373 ],
       [ 0.16518017],
       [ 0.03113988],
       [-0.01840208],
       [ 0.00686219]]), -0.0011501251054661615]
At round 6 accuracy: 0.4195238095238095
At round 6 training accuracy: 0.4287755102040816
At round 6 training loss: 0.4074869581631252
gradient difference: 0.003533277415988839
At round 6 model param: 
[array([[-2.03667249],
       [ 1.08985725],
       [ 0.81980774],
       [ 0.45159465],
       [ 0.17252334],
       [ 0.03555779],
       [-0.01629356],
       [ 0.0115549 ]]), 0.001306499055187617]
At round 7 accuracy: 0.4195238095238095
At round 7 training accuracy: 0.4287755102040816
At round 7 training loss: 0.4074621668883732
gradient difference: 0.003543073252512437
At round 7 model param: 
[array([[-2.04395294],
       [ 1.08462283],
       [ 0.81408981],
       [ 0.44684788],
       [ 0.16463051],
       [ 0.0303884 ],
       [-0.01589816],
       [ 0.00404804]]), -0.0041270676468099865]
At round 8 accuracy: 0.4195238095238095
At round 8 training accuracy: 0.4287755102040816
At round 8 training loss: 0.40755655084337505
gradient difference: 0.0035309985249073286
At round 8 model param: 
[array([[-2.02963264],
       [ 1.09723781],
       [ 0.82668835],
       [ 0.45812407],
       [ 0.1749292 ],
       [ 0.0418286 ],
       [-0.00978401],
       [ 0.01867458]]), 0.005983429295676095]
At round 9 accuracy: 0.4195238095238095
At round 9 training accuracy: 0.4287755102040816
At round 9 training loss: 0.4077546511377607
gradient difference: 0.0035514313787180494
At round 9 model param: 
[array([[-2.03890702],
       [ 1.09085897],
       [ 0.81651433],
       [ 0.45106035],
       [ 0.16926604],
       [ 0.03547938],
       [-0.01395959],
       [ 0.00917118]]), -0.0005626756963985306]
Total training time: 39.807392835617065s
At round 10 accuracy: 0.4195238095238095
At round 10 training accuracy: 0.4287755102040816
At round 10 model param: 
[array([[-2.03890702],
       [ 1.09085897],
       [ 0.81651433],
       [ 0.45106035],
       [ 0.16926604],
       [ 0.03547938],
       [-0.01395959],
       [ 0.00917118]]), -0.0005626756963985306]
