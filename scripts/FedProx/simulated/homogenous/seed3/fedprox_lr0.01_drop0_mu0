Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 3
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.4257142857142857
At round 0 training accuracy: 0.42714285714285716
At round 0 training loss: 0.6931464076042175
gradient difference: 0.0028543198126408775
At round 0 model param: 
[array([[-1.79190132],
       [ 0.8988423 ],
       [ 0.62485827],
       [ 0.39561754],
       [ 0.1569401 ],
       [ 0.12928752],
       [ 0.01071694],
       [ 0.02490204]]), 0.09309238301856178]
At round 1 accuracy: 0.4257142857142857
At round 1 training accuracy: 0.42714285714285716
At round 1 training loss: 0.4077099731990269
gradient difference: 0.0012857713672319924
At round 1 model param: 
[array([[-1.96987726],
       [ 0.98045704],
       [ 0.6891716 ],
       [ 0.42856759],
       [ 0.17467117],
       [ 0.13913458],
       [ 0.00927093],
       [ 0.02044455]]), 0.12870630455602491]
At round 2 accuracy: 0.4257142857142857
At round 2 training accuracy: 0.42714285714285716
At round 2 training loss: 0.4063666931220463
gradient difference: 0.0012393816712299418
At round 2 model param: 
[array([[-2.01194651],
       [ 0.9905416 ],
       [ 0.69749721],
       [ 0.42760221],
       [ 0.17312982],
       [ 0.14005765],
       [ 0.00499988],
       [ 0.01271463]]), 0.14515720254608563]
At round 3 accuracy: 0.4257142857142857
At round 3 training accuracy: 0.42714285714285716
At round 3 training loss: 0.40625810623168945
gradient difference: 0.0012715710033611991
At round 3 model param: 
[array([[-2.02567141e+00],
       [ 9.89191796e-01],
       [ 6.96595260e-01],
       [ 4.24402267e-01],
       [ 1.67447458e-01],
       [ 1.34613711e-01],
       [ 1.52527115e-03],
       [ 6.49135161e-03]]), 0.1529738392148699]
At round 4 accuracy: 0.4257142857142857
At round 4 training accuracy: 0.42714285714285716
At round 4 training loss: 0.40634050965309143
gradient difference: 0.001310344579413005
At round 4 model param: 
[array([[-2.02459584e+00],
       [ 9.92986688e-01],
       [ 6.95913613e-01],
       [ 4.25470718e-01],
       [ 1.68763701e-01],
       [ 1.37052364e-01],
       [ 1.97557120e-03],
       [ 9.55186172e-03]]), 0.16201432475021907]
At round 5 accuracy: 0.4257142857142857
At round 5 training accuracy: 0.42714285714285716
At round 5 training loss: 0.40628044094358173
gradient difference: 0.0012945953758036234
At round 5 model param: 
[array([[-2.02357994e+00],
       [ 9.92412891e-01],
       [ 6.97590521e-01],
       [ 4.25415827e-01],
       [ 1.69836223e-01],
       [ 1.36770721e-01],
       [-6.24196604e-04],
       [ 7.81795303e-03]]), 0.1663339797939573]
At round 6 accuracy: 0.4257142857142857
At round 6 training accuracy: 0.42714285714285716
At round 6 training loss: 0.40627763101032804
gradient difference: 0.0012920975651076342
At round 6 model param: 
[array([[-2.02756459e+00],
       [ 9.89283238e-01],
       [ 6.95542514e-01],
       [ 4.23406584e-01],
       [ 1.68960561e-01],
       [ 1.34201411e-01],
       [ 5.52870200e-04],
       [ 5.63034894e-03]]), 0.16710420378616878]
At round 7 accuracy: 0.4257142857142857
At round 7 training accuracy: 0.42714285714285716
At round 7 training loss: 0.4063139089516231
gradient difference: 0.0013074859289454186
At round 7 model param: 
[array([[-2.02349581],
       [ 0.9946949 ],
       [ 0.69616608],
       [ 0.42609395],
       [ 0.17031319],
       [ 0.13737785],
       [ 0.00528241],
       [ 0.00864973]]), 0.1721837158714022]
At round 8 accuracy: 0.4257142857142857
At round 8 training accuracy: 0.42714285714285716
At round 8 training loss: 0.4062776395252773
gradient difference: 0.001281843510391405
At round 8 model param: 
[array([[-2.02520875],
       [ 0.99211778],
       [ 0.69693305],
       [ 0.42581964],
       [ 0.1697378 ],
       [ 0.134484  ],
       [ 0.00428957],
       [ 0.00792078]]), 0.17182398161717824]
At round 9 accuracy: 0.4257142857142857
At round 9 training accuracy: 0.42714285714285716
At round 9 training loss: 0.40627507652555195
gradient difference: 0.0012889947619960276
At round 9 model param: 
[array([[-2.02122947],
       [ 0.99783868],
       [ 0.69818223],
       [ 0.42951511],
       [ 0.17248353],
       [ 0.13788075],
       [ 0.00467425],
       [ 0.01159735]]), 0.1750042044690677]
Total training time: 43.94856095314026s
At round 10 accuracy: 0.4257142857142857
At round 10 training accuracy: 0.42714285714285716
At round 10 model param: 
[array([[-2.02122947],
       [ 0.99783868],
       [ 0.69818223],
       [ 0.42951511],
       [ 0.17248353],
       [ 0.13788075],
       [ 0.00467425],
       [ 0.01159735]]), 0.1750042044690677]
