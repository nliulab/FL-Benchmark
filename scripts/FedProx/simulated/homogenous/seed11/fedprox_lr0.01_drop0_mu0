Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 11
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.42904761904761907
At round 0 training accuracy: 0.4216326530612245
At round 0 training loss: 0.6931464076042175
gradient difference: 0.0018651319893853754
At round 0 model param: 
[array([[-1.70537455],
       [ 0.86489863],
       [ 0.71616403],
       [ 0.27469329],
       [ 0.2165822 ],
       [ 0.07829468],
       [-0.02699659],
       [-0.02772411]]), 0.10807981022766658]
At round 1 accuracy: 0.42904761904761907
At round 1 training accuracy: 0.4216326530612245
At round 1 training loss: 0.42057388595172335
gradient difference: 0.0007287781097899526
At round 1 model param: 
[array([[-1.87165136],
       [ 0.93216423],
       [ 0.77469557],
       [ 0.28425596],
       [ 0.22512255],
       [ 0.07685503],
       [-0.04174229],
       [-0.05073979]]), 0.14791542610951833]
At round 2 accuracy: 0.42904761904761907
At round 2 training accuracy: 0.4216326530612245
At round 2 training loss: 0.41951594608170645
gradient difference: 0.0007500430864413972
At round 2 model param: 
[array([[-1.90513492],
       [ 0.94419825],
       [ 0.77843849],
       [ 0.28490368],
       [ 0.22170092],
       [ 0.07438021],
       [-0.04938099],
       [-0.0580425 ]]), 0.1724736796958106]
At round 3 accuracy: 0.42904761904761907
At round 3 training accuracy: 0.4216326530612245
At round 3 training loss: 0.41961867468697683
gradient difference: 0.0007545495531769564
At round 3 model param: 
[array([[-1.90999322],
       [ 0.94438179],
       [ 0.78395775],
       [ 0.28841451],
       [ 0.2240248 ],
       [ 0.07338199],
       [-0.04831351],
       [-0.05656874]]), 0.19091524609497615]
At round 4 accuracy: 0.42904761904761907
At round 4 training accuracy: 0.4216326530612245
At round 4 training loss: 0.41949139322553364
gradient difference: 0.000753539483700191
At round 4 model param: 
[array([[-1.91495025],
       [ 0.94449524],
       [ 0.77848423],
       [ 0.28407615],
       [ 0.21852577],
       [ 0.0696855 ],
       [-0.05216373],
       [-0.06572182]]), 0.19687512942722865]
At round 5 accuracy: 0.42904761904761907
At round 5 training accuracy: 0.4216326530612245
At round 5 training loss: 0.419678053685597
gradient difference: 0.0007555575077440364
At round 5 model param: 
[array([[-1.90812617],
       [ 0.9489803 ],
       [ 0.78396323],
       [ 0.28970202],
       [ 0.22523829],
       [ 0.07932414],
       [-0.04569017],
       [-0.05714625]]), 0.20884081295558385]
At round 6 accuracy: 0.42904761904761907
At round 6 training accuracy: 0.4216326530612245
At round 6 training loss: 0.419448265007564
gradient difference: 0.0007487201046952269
At round 6 model param: 
[array([[-1.91551634],
       [ 0.94249696],
       [ 0.7796116 ],
       [ 0.28140083],
       [ 0.21915609],
       [ 0.072729  ],
       [-0.05311333],
       [-0.06291165]]), 0.20617727083819254]
At round 7 accuracy: 0.42904761904761907
At round 7 training accuracy: 0.4216326530612245
At round 7 training loss: 0.41960236004420687
gradient difference: 0.0007557504647363039
At round 7 model param: 
[array([[-1.91057839],
       [ 0.94635599],
       [ 0.7829969 ],
       [ 0.28480531],
       [ 0.22230218],
       [ 0.07687655],
       [-0.05016422],
       [-0.05978048]]), 0.21107969965253556]
At round 8 accuracy: 0.42904761904761907
At round 8 training accuracy: 0.4216326530612245
At round 8 training loss: 0.4194539913109371
gradient difference: 0.0007516440246036884
At round 8 model param: 
[array([[-1.91799934],
       [ 0.93831266],
       [ 0.77837813],
       [ 0.28073151],
       [ 0.218224  ],
       [ 0.07129201],
       [-0.05502594],
       [-0.06689134]]), 0.20677519696099417]
At round 9 accuracy: 0.42904761904761907
At round 9 training accuracy: 0.4216326530612245
At round 9 training loss: 0.4197354529585157
gradient difference: 0.0007578919577497966
At round 9 model param: 
[array([[-1.91208027],
       [ 0.94561414],
       [ 0.78161774],
       [ 0.28322837],
       [ 0.22058495],
       [ 0.07257273],
       [-0.04960776],
       [-0.06044139]]), 0.21139390979494369]
Total training time: 44.39557600021362s
At round 10 accuracy: 0.42904761904761907
At round 10 training accuracy: 0.4216326530612245
At round 10 model param: 
[array([[-1.91208027],
       [ 0.94561414],
       [ 0.78161774],
       [ 0.28322837],
       [ 0.22058495],
       [ 0.07257273],
       [-0.04960776],
       [-0.06044139]]), 0.21139390979494369]
