Arguments:
	       batch_size : 10
	clients_per_round : 3
	          dataset : simulated
	     drop_percent : 0.0
	       eval_every : 1
	           folder : homogenous
	       folderSeed : 6
	    learning_rate : 0.01
	            model : mclr
	     model_params : (2,)
	               mu : 0.0
	       num_epochs : 20
	        num_iters : 1
	       num_rounds : 10
	        optimizer : fedprox
	             seed : 0
Using Federated prox to Train

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              1
-min_occurrence             0
-step                       -1
-order_by                   float_ops
-account_type_regexes       .*
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     float_ops
-output                     stdout:

==================Model Analysis Report======================

Doc:
scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.
flops: Number of float operations. Note: Please read the implementation for the math behind it.

Profile:
node name | # float_ops
_TFProfRoot (--/60 flops)
  Neg (8/8 flops)
  PGD/update_Variable/AssignSub (8/8 flops)
  PGD/update_Variable/mul (8/8 flops)
  PGD/update_Variable/mul_1 (8/8 flops)
  PGD/update_Variable/sub (8/8 flops)
  gradients/Neg_grad/Neg (8/8 flops)
  PGD/update_Variable_1/AssignSub (1/1 flops)
  PGD/update_Variable_1/mul (1/1 flops)
  PGD/update_Variable_1/mul_1 (1/1 flops)
  PGD/update_Variable_1/sub (1/1 flops)
  binary_crossentropy/sub (1/1 flops)
  binary_crossentropy/weighted_loss/Mul (1/1 flops)
  gradients/binary_crossentropy/Mean_grad/Maximum (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/Mul_grad/Mul_1 (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/Neg (1/1 flops)
  gradients/binary_crossentropy/weighted_loss/value_grad/mul (1/1 flops)
  gradients/truediv_grad/Neg (1/1 flops)

======================End of Report==========================
3 Clients in Total
Training with 3 workers ---
At round 0 accuracy: 0.4480952380952381
At round 0 training accuracy: 0.4193877551020408
At round 0 training loss: 0.6931464076042175
gradient difference: 0.002309622742957588
At round 0 model param: 
[array([[-1.79531588],
       [ 0.94155307],
       [ 0.69410698],
       [ 0.36123372],
       [ 0.19247139],
       [ 0.09738864],
       [ 0.01090206],
       [ 0.00239629]]), 0.05092115913118635]
At round 1 accuracy: 0.4480952380952381
At round 1 training accuracy: 0.4193877551020408
At round 1 training loss: 0.40729701093264986
gradient difference: 0.0018106778549180386
At round 1 model param: 
[array([[-1.99205852],
       [ 1.04080626],
       [ 0.77064086],
       [ 0.39264585],
       [ 0.2138865 ],
       [ 0.1061682 ],
       [ 0.01116547],
       [-0.00311378]]), 0.03353968555373805]
At round 2 accuracy: 0.4480952380952381
At round 2 training accuracy: 0.4193877551020408
At round 2 training loss: 0.40570737208638874
gradient difference: 0.001869677450141269
At round 2 model param: 
[array([[-2.02818547],
       [ 1.06828495],
       [ 0.79199715],
       [ 0.40597601],
       [ 0.22178085],
       [ 0.11887898],
       [ 0.01835302],
       [ 0.00492817]]), 0.02606383970539485]
At round 3 accuracy: 0.4480952380952381
At round 3 training accuracy: 0.4193877551020408
At round 3 training loss: 0.4056426244122641
gradient difference: 0.0018530319451394025
At round 3 model param: 
[array([[-2.03970931],
       [ 1.07374262],
       [ 0.79482905],
       [ 0.40715994],
       [ 0.22613318],
       [ 0.1169912 ],
       [ 0.01954116],
       [ 0.00363993]]), 0.01638306224984782]
At round 4 accuracy: 0.4480952380952381
At round 4 training accuracy: 0.4193877551020408
At round 4 training loss: 0.40563252568244934
gradient difference: 0.0018596508864325463
At round 4 model param: 
[array([[-2.03778456],
       [ 1.07754247],
       [ 0.80092846],
       [ 0.41259821],
       [ 0.22761671],
       [ 0.12228024],
       [ 0.01833221],
       [ 0.00727424]]), 0.013957014573471887]
At round 5 accuracy: 0.4480952380952381
At round 5 training accuracy: 0.4193877551020408
At round 5 training loss: 0.4056990487234933
gradient difference: 0.0018492109802131188
At round 5 model param: 
[array([[-2.0292575 ],
       [ 1.0859181 ],
       [ 0.80873668],
       [ 0.4171954 ],
       [ 0.23519889],
       [ 0.125755  ],
       [ 0.02725015],
       [ 0.01466608]]), 0.01764329363192831]
At round 6 accuracy: 0.4480952380952381
At round 6 training accuracy: 0.4193877551020408
At round 6 training loss: 0.4062209555080959
gradient difference: 0.0018144826857470694
At round 6 model param: 
[array([[-2.04655237e+00],
       [ 1.07126817e+00],
       [ 7.94048326e-01],
       [ 4.06592418e-01],
       [ 2.21882075e-01],
       [ 1.12642493e-01],
       [ 1.30597332e-02],
       [-1.59808568e-04]]), 0.003004990518093109]
At round 7 accuracy: 0.4480952380952381
At round 7 training accuracy: 0.4193877551020408
At round 7 training loss: 0.4057229033538273
gradient difference: 0.0018842000483152235
At round 7 model param: 
[array([[-2.04450612e+00],
       [ 1.07540829e+00],
       [ 7.95727559e-01],
       [ 4.07598655e-01],
       [ 2.22965036e-01],
       [ 1.15535774e-01],
       [ 1.46705072e-02],
       [ 2.32360193e-04]]), 0.003596748092344829]
At round 8 accuracy: 0.4480952380952381
At round 8 training accuracy: 0.4193877551020408
At round 8 training loss: 0.4056615574019296
gradient difference: 0.0018768668915395614
At round 8 model param: 
[array([[-2.04386737],
       [ 1.07582462],
       [ 0.79695123],
       [ 0.40504597],
       [ 0.22542854],
       [ 0.11668039],
       [ 0.01651236],
       [ 0.00528199]]), 0.0038711540400981903]
At round 9 accuracy: 0.4480952380952381
At round 9 training accuracy: 0.4193877551020408
At round 9 training loss: 0.40563773257391794
gradient difference: 0.0018677550894045916
At round 9 model param: 
[array([[-2.04479126],
       [ 1.07360726],
       [ 0.7962834 ],
       [ 0.40447021],
       [ 0.22199976],
       [ 0.11427247],
       [ 0.01750546],
       [ 0.00209139]]), 0.0018924740808350699]
Total training time: 44.6356258392334s
At round 10 accuracy: 0.4480952380952381
At round 10 training accuracy: 0.4193877551020408
At round 10 model param: 
[array([[-2.04479126],
       [ 1.07360726],
       [ 0.7962834 ],
       [ 0.40447021],
       [ 0.22199976],
       [ 0.11427247],
       [ 0.01750546],
       [ 0.00209139]]), 0.0018924740808350699]
